{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from functorch import jacrev  # Import from functorch\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import Double_Pendulum.Lumped_Mass.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.Lumped_Mass.transforms as transforms\n",
    "import Double_Pendulum.Lumped_Mass.dynamics as dynamics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'l1': 2, 'l2': 2, 'm': 3, 'g': 9.81, 'xa': 5, 'ya': 1}\n"
     ]
    }
   ],
   "source": [
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Number of samples\n",
    "q1_low  = -torch.pi/2\n",
    "q1_high =  torch.pi/2\n",
    "q2_low  = -torch.pi/2\n",
    "q2_high =  torch.pi/2\n",
    "q1_d_low  = -torch.pi/2\n",
    "q1_d_high =  torch.pi/2\n",
    "q2_d_low  = -torch.pi/2\n",
    "q2_d_high =  torch.pi/2\n",
    "\n",
    "\n",
    "n_samples = 100000\n",
    "\n",
    "# Generate uniformly distributed points for q1 and q2\n",
    "q1 = torch.linspace(q1_low, q1_high, n_samples)\n",
    "q2 = torch.linspace(q2_low, q2_high, n_samples)\n",
    "q1_d = torch.linspace(q1_d_low, q1_d_high, n_samples)\n",
    "q2_d = torch.linspace(q2_d_low, q2_d_high, n_samples)\n",
    "\n",
    "idx = torch.randperm(q2.shape[0])\n",
    "idx2 = torch.randperm(q1_d.shape[0])\n",
    "idx3 = torch.randperm(q2_d.shape[0])\n",
    "\n",
    "q2 = q2[idx]\n",
    "q1_d = q1_d[idx2]\n",
    "q2_d = q2_d[idx3]\n",
    "\n",
    "# Stack q1 and q2 to get the 2D coordinates\n",
    "points = torch.stack([q1, q2, q1_d, q2_d], axis=1)  # Shape will be (1000, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5892e-01,  4.5674e-01, -9.0392e-01,  9.6443e-01],\n",
      "        [ 9.2353e-01, -1.4327e+00, -4.5087e-01, -2.7073e-01],\n",
      "        [ 1.2334e+00, -1.3141e+00,  1.3051e+00,  1.2553e+00],\n",
      "        [-5.1662e-01, -4.8219e-01,  6.0908e-01,  7.0451e-02],\n",
      "        [-6.7958e-01,  1.3651e+00, -9.4169e-01, -3.2920e-01],\n",
      "        [-9.1908e-02,  5.4317e-01,  3.2172e-01, -3.6206e-01],\n",
      "        [-8.6164e-01,  1.2425e+00, -1.2464e-01, -2.8229e-01],\n",
      "        [-1.3327e+00,  1.0898e+00, -1.3319e-01, -1.5842e-01],\n",
      "        [-1.4402e+00, -3.0375e-01, -1.0055e+00,  1.3621e+00],\n",
      "        [-9.2943e-01,  1.1025e+00,  1.0716e+00,  1.5571e+00],\n",
      "        [-1.3445e+00, -1.3805e+00,  5.0827e-01, -9.2061e-01],\n",
      "        [ 1.1577e+00,  1.5690e+00, -1.1193e+00,  7.8530e-01],\n",
      "        [ 3.4572e-01, -5.9859e-01,  1.2621e-01, -5.5560e-02],\n",
      "        [ 1.4858e+00, -4.4845e-01,  9.6974e-01,  1.4848e+00],\n",
      "        [ 6.0544e-01,  7.0198e-01,  8.7826e-01,  1.2609e-01],\n",
      "        [ 6.0162e-03, -1.2752e+00,  9.8045e-01, -1.5315e+00],\n",
      "        [ 8.0270e-01, -8.8705e-01, -1.3042e+00,  3.0230e-01],\n",
      "        [ 1.5219e+00,  8.9811e-01,  1.2824e+00, -4.4126e-01],\n",
      "        [ 1.2723e-03, -1.4462e+00,  2.7620e-01, -8.5865e-01],\n",
      "        [-1.3269e-01,  1.4308e+00,  6.1596e-01,  1.4266e+00],\n",
      "        [-6.7261e-01, -4.8712e-01,  6.3893e-01,  8.2997e-01],\n",
      "        [-1.1924e-01,  6.7056e-01,  6.2690e-01,  3.6458e-02],\n",
      "        [ 1.3105e+00,  1.9034e-01, -5.0745e-01,  1.6178e-01],\n",
      "        [ 8.4813e-01,  1.3712e-01, -1.5427e+00, -9.2133e-01],\n",
      "        [-1.4352e+00, -7.2762e-01, -1.4297e+00, -2.1767e-01],\n",
      "        [-1.0751e+00,  1.1636e+00,  2.0373e-02,  6.0487e-01],\n",
      "        [-1.3605e+00, -4.6532e-01,  9.4533e-01,  1.5198e+00],\n",
      "        [ 1.3459e+00, -1.3796e+00,  1.2062e+00, -1.5393e+00],\n",
      "        [-8.9230e-01, -1.7617e-01, -6.7358e-01,  1.1294e+00],\n",
      "        [-4.3529e-01,  1.5186e+00,  1.3213e+00, -8.2014e-01],\n",
      "        [ 1.4626e+00,  1.4155e-01,  9.0326e-01,  2.3360e-01],\n",
      "        [ 1.5495e+00,  2.7532e-01, -1.2152e+00,  1.1867e+00],\n",
      "        [-9.0337e-02, -8.5868e-01,  1.0175e+00, -8.0567e-02],\n",
      "        [ 5.0425e-01,  7.4895e-01, -3.5108e-02,  1.2301e+00],\n",
      "        [-3.0460e-01, -1.1011e+00,  1.0008e+00,  4.7267e-01],\n",
      "        [ 4.9847e-01, -1.2385e+00,  9.9114e-01,  1.5005e+00],\n",
      "        [ 8.4518e-01, -3.1887e-03,  3.0852e-01,  3.1082e-01],\n",
      "        [-1.2606e-01,  3.2269e-01,  1.4366e+00, -7.9268e-01],\n",
      "        [-1.0008e+00,  1.5046e+00, -1.1576e+00, -5.7792e-01],\n",
      "        [-1.4472e+00,  1.0136e+00, -1.2396e+00,  1.3561e-01],\n",
      "        [-1.0840e+00,  1.4072e+00, -1.5019e-01, -2.3341e-01],\n",
      "        [-4.6952e-02, -2.1305e-01, -1.5061e+00,  3.6218e-01],\n",
      "        [-3.8732e-01, -1.2522e+00,  6.6639e-01,  1.3055e+00],\n",
      "        [-1.3093e+00, -9.9164e-01, -3.5571e-01,  4.6799e-01],\n",
      "        [-9.0009e-01,  1.4680e+00,  1.5509e-01, -3.8446e-01],\n",
      "        [-1.1524e+00, -7.0010e-01, -1.4887e+00,  2.0656e-02],\n",
      "        [ 2.1355e-01,  1.2329e+00,  5.9020e-01,  1.4269e+00],\n",
      "        [-1.3627e+00,  2.6991e-01, -2.1993e-01,  6.2799e-01],\n",
      "        [ 1.2942e+00, -2.7695e-01,  2.8867e-01,  6.1618e-01],\n",
      "        [ 5.3378e-01, -1.5680e+00, -1.3595e+00,  5.7195e-01],\n",
      "        [ 9.8175e-03,  1.2930e+00, -1.4219e+00,  9.9327e-01],\n",
      "        [-1.2264e+00, -3.5932e-01,  8.2139e-01,  3.6071e-01],\n",
      "        [-1.1127e+00, -9.3920e-01, -5.9755e-01,  2.4761e-01],\n",
      "        [-7.0503e-01,  1.4945e+00, -9.2507e-01, -7.9972e-01],\n",
      "        [-2.3209e-01,  1.2339e+00, -5.0846e-01,  1.1910e+00],\n",
      "        [-1.0047e+00,  1.4574e+00,  8.0389e-01, -1.1486e+00],\n",
      "        [ 1.3234e+00, -1.5422e+00,  5.0789e-01, -1.5653e+00],\n",
      "        [ 8.2334e-01, -6.8341e-01, -3.8694e-01, -1.4963e+00],\n",
      "        [ 4.7905e-01, -7.5143e-01,  1.2608e+00, -1.3215e+00],\n",
      "        [-1.4742e+00, -1.3708e+00,  1.3646e+00, -1.3097e+00],\n",
      "        [ 1.1978e+00,  4.6328e-01,  1.1916e+00, -1.3603e+00],\n",
      "        [ 1.1434e+00,  5.4911e-01, -5.9381e-01,  3.6520e-01],\n",
      "        [-1.1804e+00,  3.6710e-02, -1.1494e+00,  1.8741e-01],\n",
      "        [-1.4915e+00,  1.8019e-01, -6.9567e-01,  5.7632e-01]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kian/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/cuda/__init__.py:128: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "# Use TensorDataset to create the dataset\n",
    "dataset = TensorDataset(points)\n",
    "\n",
    "# Create the DataLoader with batch size and shuffling\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, \n",
    "                        batch_size=batch_size, \n",
    "                        shuffle=True,\n",
    "                        num_workers=0,\n",
    "                        pin_memory=True)\n",
    "\n",
    "\n",
    "# Example usage: iterate through the DataLoader\n",
    "for batch in dataloader:\n",
    "    print(batch[0])\n",
    "    break  # Just to show one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718026f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9b5de6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinCosLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SinCosLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply sin() and cos() to both coordinates\n",
    "        x_sin = torch.sin(x)\n",
    "        x_cos = torch.cos(x)\n",
    "        x_sin_cos_shape = (x.shape[0], x.shape[1]*2)\n",
    "        x_sin_cos = torch.empty(x_sin_cos_shape, dtype=x_sin.dtype, device=x.device)\n",
    "        x_sin_cos[:,0::2] = x_sin\n",
    "        x_sin_cos[:,1::2] = x_cos\n",
    "        return x_sin_cos\n",
    "    \n",
    "# Custom layer to reverse the interleaved sin() and cos() back to original coordinates\n",
    "class InverseSinCosLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InverseSinCosLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x contains interleaved sin() and cos() values\n",
    "        # Assuming input is of shape (batch_size, 4) for 2D coordinates\n",
    "        sin_vals = x[:, 0::2]  # Extract sin values\n",
    "        cos_vals = x[:, 1::2]  # Extract cos values\n",
    "\n",
    "        # Use atan2 to recover the original angles from sin and cos\n",
    "        original_coords = torch.atan2(sin_vals, cos_vals)\n",
    "        return original_coords\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            #SinCosLayer(),\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 1)#,\n",
    "            #InverseSinCosLayer()\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            #SinCosLayer(),\n",
    "            nn.Linear(2, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4, 2)#,\n",
    "            #InverseSinCosLayer()\n",
    "        )\n",
    "    \n",
    "    def forward(self, q):\n",
    "        theta_1 = transforms.analytic_theta_1(rp, q).unsqueeze(1)\n",
    "        theta_2 = self.encoder(q)\n",
    "        theta = torch.stack((theta_1, theta_2), dim=1).squeeze(2)\n",
    "        q_hat = self.decoder(theta)\n",
    "        return(theta, q_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c18c8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got to 1\n",
      "got to 2\n",
      "got to 3\n",
      "tensor([[1.5696, 0.1522],\n",
      "        [0.0032, 0.0091]], grad_fn=<SelectBackward0>)\n",
      "tensor([[  0.6593, -11.0258],\n",
      "        [ -0.2291, 113.6901]], grad_fn=<SelectBackward0>)\n",
      "tensor([[  0.6593,  -0.2291],\n",
      "        [-11.0258, 113.6901]], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:32: FutureWarning: We've integrated functorch into PyTorch. As the final step of the integration, `functorch.jacrev` is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use `torch.func.jacrev` instead; see the PyTorch 2.0 release notes and/or the `torch.func` migration guide for more details https://pytorch.org/docs/main/func.migrating.html\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:47\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/_functorch/apis.py:201\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/thesis/lib/python3.12/site-packages/torch/_functorch/vmap.py:480\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    477\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    478\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 480\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/Documents/Thesis/ICS_fork/ics-pa-sv/Kian_code/Double_Pendulum/Lumped_Mass/dynamics.py:24\u001b[0m, in \u001b[0;36mdynamical_matrices\u001b[0;34m(rp, q, q_d)\u001b[0m\n\u001b[1;32m     21\u001b[0m s12 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(q[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mq[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     22\u001b[0m c12 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcos(q[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m-\u001b[39mq[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m---> 24\u001b[0m M_q \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                    \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc12\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc12\u001b[49m\u001b[43m,\u001b[49m\u001b[43m      \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ml2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m     \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m C_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     30\u001b[0m     [[\u001b[38;5;241m0\u001b[39m,                    rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m q_d[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m s12],\n\u001b[1;32m     31\u001b[0m      [\u001b[38;5;241m-\u001b[39mrp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m q_d[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m s12,      \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m     32\u001b[0m      )\n\u001b[1;32m     34\u001b[0m G_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     35\u001b[0m     [[rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m c1],\n\u001b[1;32m     36\u001b[0m      [rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ml2\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m rp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m c2]]\n\u001b[1;32m     37\u001b[0m \n\u001b[1;32m     38\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Autoencoder().to(device)  # Move model to GPU\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "#scheduler.get_last_lr()\n",
    "\n",
    "\n",
    "num_epochs = 30\n",
    "lambda_reg = 1e-4\n",
    "outputs = []\n",
    "\n",
    "# Define a function that takes `q` as input and returns `theta`\n",
    "def compute_theta(q):\n",
    "    return model(q)[0]  # This will return `theta`\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (batch) in dataloader:\n",
    "        print(\"got to 1\")\n",
    "        q = batch[0][:, 0:2].to(device)\n",
    "        print(\"got to 2\")\n",
    "        q_d = batch[0][:, 2:4].to(device)\n",
    "        print(\"got to 3\")\n",
    "        theta, q_hat = model(q)\n",
    "        \n",
    "        #Jh_func = torch.vmap(jacrev(compute_theta)) Doesn't work due to batch handling in transforms.py\n",
    "        Jh_func = jacrev(compute_theta)\n",
    "        \n",
    "        Jh_messy = Jh_func(q)\n",
    "        \n",
    "        Jh_blocks = Jh_messy.view(batch_size, 2, batch_size, 2)  # [64, 2, 64, 2]\n",
    "        Jh = torch.stack([Jh_blocks[i, :, i, :] for i in range(batch_size)])\n",
    "        \n",
    "        Jh_inv = torch.linalg.inv(Jh)\n",
    "        Jh_invtrans = Jh_inv.transpose(1, 2)\n",
    "        \n",
    "        print(Jh[0])\n",
    "        print(Jh_inv[0])\n",
    "        print(Jh_invtrans[0])\n",
    "        \n",
    "        matrices_vmap = torch.vmap(dynamics.dynamical_matrices, (None, 0, 0))\n",
    "        Mq, Cq, Gq = matrices_vmap(rp, q, q_d)\n",
    "        \n",
    "\n",
    "        loss_reconstruction = criterion(q_hat, q)\n",
    "        loss_theta = criterion(theta, q)\n",
    "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
    "        loss = loss_reconstruction + loss_theta #+ lambda_reg * l1_norm\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "    #print(scheduler.get_last_lr())\n",
    "        \n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.9f}')\n",
    "    print(\"l1 norm loss:\", (l1_norm*lambda_reg).item())\n",
    "    outputs.append((epoch, q, q_hat, theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ea012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "max_neurons = 8\n",
    "blank_layer = [None for _ in range(max_neurons)]\n",
    "\n",
    "table_layers = []\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    layer = param.data\n",
    "    num_parallel = layer.shape[0]\n",
    "    side_padding = int((max_neurons - num_parallel)/2)\n",
    "    \n",
    "    if idx % 2 == 0:\n",
    "        \n",
    "        table_layer = blank_layer.copy()\n",
    "        table_layer[0] = \"weights\" + str(idx//2+1)\n",
    "        table_layers.append(table_layer)\n",
    "        for i in range(layer.shape[1]):\n",
    "            table_layer = blank_layer.copy()\n",
    "            for j in range(num_parallel):\n",
    "                table_layer[j+side_padding] = '{:.2e}'.format(layer[j][i].item())\n",
    "            table_layers.append(table_layer)\n",
    "        table_layers.append(blank_layer)\n",
    "            \n",
    "    else:  \n",
    "        \n",
    "        table_layer = blank_layer.copy()\n",
    "        table_layer[0] = \"bias\" + str(idx//2+1)\n",
    "        table_layers.append(table_layer)\n",
    "        table_layer = blank_layer.copy()\n",
    "        for j in range(num_parallel):\n",
    "            table_layer[j+side_padding] = '{:.2e}'.format(layer[j].item())\n",
    "        table_layers.append(table_layer)\n",
    "        table_layers.append(blank_layer)\n",
    "\n",
    "numeric_values = np.zeros((len(table_layers), max_neurons))\n",
    "for i, row in enumerate(table_layers):\n",
    "    for j, item in enumerate(row):\n",
    "        if item not in (None, \"weights1\", \"weights2\", \"bias1\", \"bias2\"):  # Replace with relevant layer names\n",
    "            try:\n",
    "                numeric_values[i, j] = (float(item))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "min_val, max_val = numeric_values.min(), numeric_values.max()\n",
    "\n",
    "\n",
    "# Step 2: Apply a logarithmic transformation, setting a small threshold to avoid log(0)\n",
    "threshold = 1e-5\n",
    "log_values = np.log10(np.clip(np.abs(numeric_values), threshold, None))\n",
    "\n",
    "# Normalize the log-scaled values to range between 0 and 1\n",
    "normalized_values = (log_values - log_values.min()) / (log_values.max() - log_values.min())\n",
    "colors = plt.cm.Blues(normalized_values)\n",
    "\n",
    "        \n",
    "# Plot the table\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table\n",
    "table = plt.table(cellText=table_layers, cellColours=colors, loc='center', cellLoc='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32c736",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d7144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (thesis)",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
