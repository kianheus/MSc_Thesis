{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import functorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import Double_Pendulum.Lumped_Mass.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.Lumped_Mass.transforms as transforms\n",
    "import Double_Pendulum.Lumped_Mass.dynamics as dynamics\n",
    "import Learning.loss_terms as loss_terms\n",
    "#import Plotting.plotters_h1h2 as plotters_h1h2\n",
    "import Learning.training_data as training_data\n",
    "import Plotting.theta_visualiser as theta_visualizer\n",
    "\n",
    "import Models.autoencoders as autoencoders\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "print(rp)\n",
    "\n",
    "mapping_functions = (transforms.analytic_theta_1, transforms.analytic_theta_2)\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, mapping_functions=mapping_functions)\n",
    "#th_plotter.make_figure(\"theta_subset_5_2_image.png\")\n",
    "#th_plotter.make_animation(\"theta_subset_5_2.mp4\", duration = 4, fps = 20, stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_points(q1_split, clockwise = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a set of [q1, q2] points based on \"q1_split\" limits on q1 and q2.\n",
    "    The limits on q2 depend on whether a clockwise or counterclockwise dataset is selected.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Retrieve training points\n",
    "    points = training_data.points.to(device)\n",
    "    \n",
    "    # Mask to retrieve only the counterclockwise points\n",
    "    width_mask = (points[:,0] >= q1_split[0]) & (points[:,0] <= q1_split[1])\n",
    "    ccw_mask = ((points[:,1] >= points[:,0]) & \n",
    "                  (points[:,1] <= points[:,0] + torch.pi)) #| ((points[:,1] >= points[:,0] - 2*torch.pi) &\n",
    "                  #(points[:,1] <= points[:,0] - torch.pi))\n",
    "    \n",
    "    # Mask to retrieve only the clockwise points\n",
    "    cw_mask = ((points[:,1] >= points[:,0] - torch.pi) & (points[:,1] <= points[:,0]))\n",
    "\n",
    "    if clockwise:\n",
    "        final_mask = width_mask & cw_mask\n",
    "    else:\n",
    "        final_mask = width_mask & ccw_mask\n",
    "    \n",
    "    points = points[final_mask]\n",
    "    points = points[0:3000]\n",
    "\n",
    "    if points.size(0) < 3000:\n",
    "        print(\"Warning: Only\", points.size(0), \"points in dataset.\")\n",
    "\n",
    "    return(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plotter(points, extend = None):\n",
    "\n",
    "   \"\"\" \n",
    "   Simple plotter function which visualizes the points used for training of the Autoencoder. \n",
    "   \"\"\"\n",
    "    \n",
    "   plt.figure(figsize=(4, 4))\n",
    "   plt.scatter(points[:, 0].cpu().numpy(), points[:, 1].cpu().numpy(), alpha=0.6, edgecolors='k', s=20)\n",
    "   plt.title('Scatter Plot of q1 vs q2')\n",
    "   plt.xlabel('q1')\n",
    "   plt.ylabel('q2')\n",
    "   plt.xlim(-2*torch.pi, 2*torch.pi)\n",
    "   plt.ylim(-2*torch.pi, 2*torch.pi)\n",
    "   plt.grid(True)\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_points = mask_points((-torch.pi, torch.pi), clockwise = False)\n",
    "deshifted_points = transforms.wrap_to_pi(masked_points.clone())\n",
    "points_plotter(masked_points, extend=\"ccw\")\n",
    "points_plotter(deshifted_points, extend=\"ccw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, rp):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 8),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(8, 2)\n",
    "        )\n",
    "        \n",
    "        self.rp = rp\n",
    "        \n",
    "    def encoder_theta_1_ana(self, q):\n",
    "        theta_1 = transforms.analytic_theta_1(self.rp, q).unsqueeze(0)\n",
    "        return theta_1, theta_1\n",
    "    \n",
    "    \n",
    "    #This function is not used in the forward pass, but is useful for comparing learned to analytic theta_2\n",
    "    def encoder_theta_2_ana(self, q):\n",
    "        theta_2 = transforms.analytic_theta_2(self.rp, q).unsqueeze(0)\n",
    "        return theta_2, theta_2\n",
    "    \n",
    "        \n",
    "    def theta_ana(self, q):\n",
    "        theta_1_ana, _ = torch.vmap(self.encoder_theta_1_ana)(q)\n",
    "        theta_2_ana, _ = torch.vmap(self.encoder_theta_2_ana)(q)\n",
    "        theta_ana = torch.cat((theta_1_ana, theta_2_ana), dim=1)\n",
    "        return theta_ana\n",
    "    \n",
    "    def theta_1_single(self, q):\n",
    "        theta_1 = transforms.analytic_theta_1(self.rp, q).unsqueeze(0)\n",
    "        return theta_1\n",
    "\n",
    "    def encoder_nn(self, q):\n",
    "        theta_2 = self.encoder(q)\n",
    "        return theta_2, theta_2\n",
    "    \n",
    "    def theta_2_single(self, q):\n",
    "        theta_2 = self.encoder(q)\n",
    "        return theta_2\n",
    "    \n",
    "    def decoder_nn(self, theta):\n",
    "        q_hat = self.decoder(theta)\n",
    "        return q_hat, q_hat\n",
    "    \n",
    "    def forward(self, q):\n",
    "        \n",
    "        J_h_1, theta_1 = torch.vmap(torch.func.jacfwd(self.encoder_theta_1_ana, has_aux=True))(q)\n",
    "        \n",
    "        J_h_2, theta_2 = torch.vmap(torch.func.jacfwd(self.encoder_nn, has_aux=True))(q)\n",
    "\n",
    "\n",
    "        J_h_2_ana, theta_2_ana = torch.vmap(torch.func.jacfwd(self.encoder_theta_2_ana, has_aux=True))(q)\n",
    "        \n",
    "        theta = torch.stack((theta_1, theta_2), dim=1).squeeze(2)\n",
    "        J_h = torch.cat((J_h_1, J_h_2), dim=1).float()\n",
    "        J_h_ana = torch.cat((J_h_1.float(), J_h_2_ana.float()), dim=1)\n",
    "\n",
    "        J_h_dec, q_hat = torch.vmap(torch.func.jacfwd(self.decoder_nn, has_aux=True))(theta)\n",
    "        J_h_dec = J_h_dec.float()\n",
    "\n",
    "        return(theta, J_h, q_hat, J_h_dec, J_h_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib widget\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "save_directory = os.path.join(os.getcwd(), \"Models\")\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "file_name = f\"Lumped_Mass_{current_time}.pth\"\n",
    "file_path = os.path.join(save_directory, file_name)\n",
    "load_path = os.path.normpath(\"/home/kian/Documents/Thesis/ICS_fork/ics-pa-sv/Kian_code/Models/Lumped_Mass_202411271337.pth\")\n",
    "\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "num_epochs = 501\n",
    "\n",
    "print(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fun(q, q_hat, m_matrix, input_matrix, J_h, J_h_dec):\n",
    "    l_recon = F.mse_loss(q, q_hat, reduction=\"sum\")\n",
    "    J_h_inv = J_h_dec\n",
    "    J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "    J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "    J_ortho = J_h @ J_h_trans\n",
    "    J_ortho_inv = J_h_inv @ J_h_inv_trans\n",
    "    \n",
    "    #identity_matrix = J_h @ J_h_inv  # Batch-wise multiplication\n",
    "    #unitary_target = torch.eye(identity_matrix.size(-1), device=identity_matrix.device).expand(identity_matrix.size(0), -1, -1)\n",
    "\n",
    "\n",
    "    # Enforce that decoder Jacobian is the inverse of the encoder Jacobian\n",
    "    ## USE ONLY WHEN J_h_inv == J_h_dec\n",
    "    #l_J_inv = torch.sum((torch.eye(identity_matrix.size(-1), device=identity_matrix.device) - identity_matrix)**2)  # Frobenius norm\n",
    "    #l_J_ortho_inv = torch.sum((torch.eye(identity_matrix.size(-1), device=identity_matrix.device) - J_ortho_inv)**2)  # Frobenius norm\\n\",\n",
    "\n",
    "    # Enforce that the Jacobian is orthogonal\n",
    "    l_J_ortho = torch.sum((torch.eye(J_ortho.size(-1), device=J_ortho.device) - J_ortho)**2)  # Frobenius norm\n",
    "\n",
    "    diag_matrix = J_h_inv_trans @ m_matrix @ J_h_inv  # Batch-wise diagonalization\n",
    "\n",
    "    ## Enforce inertial decoupling (diagonal loss)\n",
    "    #off_dia = diag_matrix[:, 0, 1] \n",
    "    #trace = torch.einsum('bii->b', diag_matrix)\n",
    "    #M_th_ratio = (off_dia/trace)\n",
    "    #l_diag = nn.MSELoss()(M_th_ratio, torch.zeros((M_th_ratio.size())).to(device))\n",
    "\n",
    "    \n",
    "    # Enforce inertial decoupling\n",
    "    ## Slight concern is that this sends all values of M_th to 0. Hopefully l_J_ortho avoids this\n",
    "    # NOTE: This is a very inefficient way of just taking the off diagonal value, squaring it and doubling it\n",
    "    dia_mask = torch.eye(2).unsqueeze(0).repeat(diag_matrix.size(0), 1, 1)\n",
    "    l_diag = torch.sum((diag_matrix - diag_matrix * dia_mask)**2)  # Penalize off-diagonal terms\n",
    "\n",
    "    ## input decoupling loss\n",
    "    input_x = J_h_inv_trans @ input_matrix\n",
    "    l_input = torch.sum((input_x[:, 1]**2)) #+ torch.sum(((input_x[:, 0]-1)**2))\n",
    "    loss = l_recon + l_diag + l_input + l_J_ortho #+ l_J_inv + l_J_ortho_inv\n",
    "    loss_terms = torch.tensor([l_recon, l_J_ortho, l_diag, l_input])\n",
    "\n",
    "    return loss, loss_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ion()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "start_time = time.time()\n",
    "\n",
    "model = Autoencoder(rp).to(device)  # Move model to GPU\n",
    "#model.load_state_dict(torch.load(load_path, weights_only=True))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-6)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5 ** (1 / num_epochs))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_loss_terms = torch.zeros(4)\n",
    "\n",
    "    for index, (q, M_q, A_q) in enumerate(train_dataloader):\n",
    "        #batch_size = batch[0].shape[0]\n",
    "        q = q.to(device)\n",
    "        #q.requires_grad = True\n",
    "        #q_d = batch[0].to(device)\n",
    "\n",
    "        M_q = M_q.to(device)\n",
    "        A_q = A_q.to(device)\n",
    "        \n",
    "        theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)  \n",
    "        theta_ana = model.theta_ana(q)\n",
    "                \n",
    "        loss, loss_terms = loss_fun(q, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "        #loss, loss_terms = loss_cheat(theta, theta_ana, J_h, J_h_ana, q, q_hat, J_h_dec)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        train_loss_terms += loss_terms\n",
    "    train_loss /= len(train_dataloader.dataset)\n",
    "    train_loss_terms /= len(train_dataloader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_loss_terms = torch.zeros(4)\n",
    "    with torch.no_grad():\n",
    "      for index, (q, M_q, A_q) in enumerate(val_dataloader):\n",
    "        q = q.to(device)\n",
    "        M_q = M_q.to(device)\n",
    "        A_q = A_q.to(device)\n",
    "\n",
    "        theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)\n",
    "        theta_ana = model.theta_ana(q)\n",
    "\n",
    "        loss, loss_terms = loss_fun(q, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "        #loss, loss_terms = loss_cheat(theta, theta_ana, J_h, J_h_ana, q, q_hat, J_h_dec)\n",
    "\n",
    "        J_h_inv = torch.linalg.inv(J_h)\n",
    "        J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "        M_th = J_h_inv_trans @ M_q @ J_h_inv \n",
    "\n",
    "        val_loss += loss.item()\n",
    "        val_loss_terms += loss_terms\n",
    "    val_loss /= len(val_dataloader.dataset)\n",
    "    val_loss_terms /= len(val_dataloader.dataset)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    epoch_duration = time.time() - start_time\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Duration: {epoch_duration:.2f} seconds'\n",
    "            )\n",
    "\n",
    "        print(\n",
    "            f\"recon: {val_loss_terms[0].item():.4f}, \"\n",
    "            f\"J_ortho: {val_loss_terms[1].item():.4f}, \"\n",
    "            f\"diag: {val_loss_terms[2].item():.4f}, \"\n",
    "            f\"input: {val_loss_terms[3].item():.4f}\"\n",
    "        )\n",
    "        print(M_th[0])\n",
    "\n",
    "#torch.save(model.state_dict(), file_path)\n",
    "#print(f\"Model parameters saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_functions = (model.theta_1_single, model.theta_2_single)\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, mapping_functions=mapping_functions)\n",
    "th_plotter.make_figure(\"theta_learned_5_2_image.png\")\n",
    "th_plotter.make_animation(\"theta_learned_5_2.mp4\", duration = 4, fps = 20, stride = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "485b2aea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mplotters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_decoupling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Thesis/ICS_fork/ics-pa-sv/Kian_code/Plotting/plotters.py:134\u001b[0m, in \u001b[0;36mplot_decoupling\u001b[0;34m(model, device, rp, epoch)\u001b[0m\n\u001b[1;32m    131\u001b[0m q1_grid, q2_grid \u001b[38;5;241m=\u001b[39m plot_data()\n\u001b[1;32m    132\u001b[0m q_grid_flat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([q1_grid\u001b[38;5;241m.\u001b[39mflatten(), q2_grid\u001b[38;5;241m.\u001b[39mflatten()], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Shape: (N, 2)\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m theta, J_h, q_hat, J_h_ana \u001b[38;5;241m=\u001b[39m model(q_grid_flat)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mtry:\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03m    J_h_inv = torch.linalg.inv(J_h).to(device)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03mexcept:\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m J_h_inv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mpinv(J_h)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "epoch = 1\n",
    "plotters.plot_decoupling(model, device, rp, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_test = torch.tensor([[2, -0.]]).to(device)\n",
    "theta, J_h, q_hat, J_h_ana = model(q_test)\n",
    "\n",
    "J_h_inv = torch.linalg.pinv(J_h)\n",
    "J_h_inv_trans = J_h_inv.transpose(1, 2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"J_h:\\n\", J_h.detach().cpu().numpy()[0])\n",
    "print(\"J_h_inv:\\n\", J_h_inv.detach().cpu().numpy()[0])\n",
    "\n",
    "M_q, C_q, G_q = dynamics.dynamical_matrices(rp, q_test[0], q_test[0])\n",
    "M_q = M_q.unsqueeze(0)\n",
    "M_th, C_th, G_th = transforms.transform_dynamical_matrices(M_q, C_q, G_q, J_h, device)\n",
    "print(\"M_q:\", M_q)\n",
    "print(\"M_th:\\n\", M_th)\n",
    "\n",
    "\n",
    "off_dia = M_th[:, 1,0]\n",
    "diag_elements = M_th[:, [0, 1], [0, 1]]\n",
    "diag_product = torch.sqrt(diag_elements[:, 0] * diag_elements[:, 1] + 1e-6)\n",
    "M_th_ratio = off_dia/diag_product\n",
    "print(\"M_th_ratio:\", M_th_ratio)\n",
    "\n",
    "print(\"M_q:\\n\", M_q)\n",
    "\n",
    "print(\"J_h_ana:\\n\", J_h_ana.detach().cpu().numpy()[0], \"\\n\")\n",
    "J_h_ana_inv = torch.linalg.pinv(J_h_ana)\n",
    "J_h_ana_inv_trans = J_h_ana_inv.transpose(1,2)\n",
    "\n",
    "print(\"J_h_ana_inv:\\n\", J_h_ana_inv.detach().cpu().numpy()[0], \"\\n\")\n",
    "\n",
    "\n",
    "M_th_ana, C_th_ana, G_th_ana = dynamics.dynamical_matrices_th(rp, q_test[0], q_test[0]) \n",
    "print(\"M_th_ana:\", M_th_ana.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract epochs and losses\n",
    "epochs = [entry[0] for entry in outputs]\n",
    "losses = [entry[1].item() for entry in outputs]\n",
    "\n",
    "# Plot loss as a function of epoch\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs, losses, marker='', label='Training Loss')\n",
    "plt.title('Training Loss vs. Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ea012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "max_neurons = 4\n",
    "blank_layer = [None for _ in range(max_neurons)]\n",
    "\n",
    "table_layers = []\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    layer = param.data\n",
    "    num_parallel = layer.shape[0]\n",
    "    side_padding = int((max_neurons - num_parallel)/2)\n",
    "    \n",
    "    if idx % 2 == 0:\n",
    "        \n",
    "        table_layer = blank_layer.copy()\n",
    "        table_layer[0] = \"weights\" + str(idx//2+1)\n",
    "        table_layers.append(table_layer)\n",
    "        for i in range(layer.shape[1]):\n",
    "            table_layer = blank_layer.copy()\n",
    "            for j in range(num_parallel):\n",
    "                table_layer[j+side_padding] = '{:.2e}'.format(layer[j][i].item())\n",
    "            table_layers.append(table_layer)\n",
    "        table_layers.append(blank_layer)\n",
    "            \n",
    "    else:  \n",
    "        \n",
    "        table_layer = blank_layer.copy()\n",
    "        table_layer[0] = \"bias\" + str(idx//2+1)\n",
    "        table_layers.append(table_layer)\n",
    "        table_layer = blank_layer.copy()\n",
    "        for j in range(num_parallel):\n",
    "            table_layer[j+side_padding] = '{:.2e}'.format(layer[j].item())\n",
    "        table_layers.append(table_layer)\n",
    "        table_layers.append(blank_layer)\n",
    "\n",
    "numeric_values = np.zeros((len(table_layers), max_neurons))\n",
    "for i, row in enumerate(table_layers):\n",
    "    for j, item in enumerate(row):\n",
    "        if item not in (None, \"weights1\", \"weights2\", \"bias1\", \"bias2\"):  # Replace with relevant layer names\n",
    "            try:\n",
    "                numeric_values[i, j] = (float(item))\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "min_val, max_val = numeric_values.min(), numeric_values.max()\n",
    "\n",
    "\n",
    "# Step 2: Apply a logarithmic transformation, setting a small threshold to avoid log(0)\n",
    "threshold = 1e-5\n",
    "log_values = np.log10(np.clip(np.abs(numeric_values), threshold, None))\n",
    "\n",
    "# Normalize the log-scaled values to range between 0 and 1\n",
    "normalized_values = (log_values - log_values.min()) / (log_values.max() - log_values.min())\n",
    "colors = plt.cm.Blues(normalized_values)\n",
    "\n",
    "        \n",
    "# Plot the table\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.axis('tight')\n",
    "ax.axis('off')\n",
    "\n",
    "# Create table\n",
    "table = plt.table(cellText=table_layers, cellColours=colors, loc='center', cellLoc='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e32c736",
   "metadata": {},
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5d7144",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder2(nn.Module):\n",
    "    def __init__(self, rp):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(3, 4),\n",
    "            #nn.Sigmoid(),\n",
    "            #nn.Linear(4, 4)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            #nn.Linear(4, 4),\n",
    "            #nn.Sigmoid(),\n",
    "            nn.Linear(4, 3),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(3, 2)\n",
    "        )\n",
    "        \n",
    "        self.rp = rp\n",
    "\n",
    "    def encoder_nn(self, q):\n",
    "        latent = self.encoder(q)\n",
    "        J_h_inv_1 = latent[:,0:2]\n",
    "        J_h_inv_2 = latent[:,2:4]\n",
    "        J_h_inv = torch.stack((J_h_inv_1, J_h_inv_2), dim=1)\n",
    "        return J_h_inv, latent\n",
    "    \n",
    "    def forward(self, q):\n",
    "        J_h_inv, latent = self.encoder_nn(q)\n",
    "\n",
    "        q_hat = self.decoder(latent)\n",
    "        return J_h_inv, q_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e337d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%matplotlib widget\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "save_directory = os.path.join(os.getcwd(), \"Models\")\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "file_name = f\"Lumped_Mass_{current_time}.pth\"\n",
    "file_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "load_path = os.path.normpath(\"/home/kian/Documents/Thesis/ICS_fork/ics-pa-sv/Kian_code/Models/Lumped_Mass_202411271337.pth\")\n",
    "\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "num_epochs = 2401\n",
    "\n",
    "print(file_path)\n",
    "model = Autoencoder2(rp).to(device)  # Move model to GPU\n",
    "#model.load_state_dict(torch.load(load_path, weights_only=True))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)#, weight_decay=1e-6)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=1e-2, alpha=0.99, eps=1e-08)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1 ** (1 / num_epochs))\n",
    "\n",
    "\n",
    "\n",
    "l_weights = [1,\n",
    "             1,\n",
    "             1e-1,\n",
    "             1e-2]\n",
    "outputs = []\n",
    "\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for index, batch in enumerate(train_dataloader):\n",
    "        batch_size = batch[0].shape[0]\n",
    "        q = batch[0][:, 0:2].to(device)\n",
    "        q.requires_grad = True\n",
    "        q_d = batch[0][:, 2:4].to(device)\n",
    "        \n",
    "        J_h_inv, q_hat = model(q)  \n",
    "        J_h_inv_trans = J_h_inv.transpose(1,2)\n",
    "                \n",
    "        matrices_vmap = torch.vmap(dynamics.dynamical_matrices, \n",
    "                                   in_dims=(None, 0, 0))\n",
    "\n",
    "        M_q, C_q, G_q = matrices_vmap(rp, q, q_d)\n",
    "        \n",
    "        M_th, C_th, G_th = transforms.transform_dynamical_from_inverse(M_q, C_q, G_q, J_h_inv, J_h_inv_trans)      \n",
    "        \n",
    "        #loss_reconstruction = loss_terms.loss_reconstruction(q, q_hat)\n",
    "        loss_diagonality_geo_mean = loss_terms.loss_diagonality_geo_mean(M_th, batch_size, device)\n",
    "        loss_diagonality_trace = loss_terms.loss_diagonality_trace(M_th, batch_size, device)\n",
    "        loss_diagonality_smallest = loss_terms.loss_diagonality_smallest(M_th, batch_size, device)\n",
    "        ### Use J@J^T = eye to avoid needing to calculate the Jacobian inverse for efficiency. \n",
    "        #loss_J_h_unitary = loss_terms.loss_J_h_unitary(J_h, batch_size, device)\n",
    "        #loss_J_h_cheat = loss_terms.loss_J_h_cheat(J_h, J_h_ana)\n",
    "        #loss_M_th_cheat = loss_terms.loss_M_th_cheat(M_th, rp, q, q_d, batch_size)\n",
    "        #l1_norm = loss_terms.loss_l1(model)\n",
    "        \n",
    "        #loss_diagonality = 10 * loss_diagonality_geo_mean + loss_diagonality_smallest + 100 * loss_diagonality_trace\n",
    "        loss_diagonality = loss_diagonality_geo_mean\n",
    "\n",
    "\n",
    "        loss = loss_diagonality #+ 0.2 * loss_J_h_unitary\n",
    "\n",
    "        \n",
    "        #loss = loss_J_h_cheat\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    " \n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print(f'Epoch:{epoch+1}, Loss:{loss.item():.9f}')#, LR:{scheduler.get_last_lr():.7f}')\n",
    "        print(\"Weighted loss_diagonality_geo_mean:\", loss_diagonality_geo_mean)\n",
    "        #print(\"Weighted loss_diagonality_smallest:\", loss_diagonality_smallest)\n",
    "        #print(\"Weighted loss_diagonality_trace:\", 100 * loss_diagonality_trace)\n",
    "        #print(\"Weighted loss Jh unitary:\", 0.2 * loss_J_h_unitary)\n",
    "    if epoch % 1200 == 0 and epoch > 0:\n",
    "        #plotters.plot_h2(model, device, rp, epoch)\n",
    "        #plotters.plot_J_h(model, device, rp, epoch, plot_index = 0)\n",
    "        #plotters.plot_J_h(model, device, rp, epoch, plot_index = 1)\n",
    "        plotters.plot_decoupling_inv(model, device, rp, epoch)\n",
    "    scheduler.step()\n",
    "\n",
    "    outputs.append((epoch, loss, q, q_hat, J_h_inv, M_th))\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), file_path)\n",
    "print(f\"Model parameters saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
