{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import Double_Pendulum.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.transforms as transforms\n",
    "import Double_Pendulum.dynamics as dynamics\n",
    "\n",
    "import training_data as training_data\n",
    "import Plotting.theta_visualiser as theta_visualizer\n",
    "\n",
    "import autoencoders\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "rp['m1'] = 0.0\n",
    "print(rp)\n",
    "\n",
    "train_clockwise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_points(q0_split, clockwise = False):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a set of [q0, q1] points based on \"q0_split\" limits on q0 and q1.\n",
    "    The limits on q1 depend on whether a clockwise or counterclockwise dataset is selected.\n",
    "    \"\"\"   \n",
    "\n",
    "    # Retrieve training points\n",
    "    points = training_data.points.to(device)\n",
    "    \n",
    "    # Mask to retrieve only the counterclockwise points\n",
    "    width_mask = (points[:,0] >= q0_split[0]) & (points[:,0] <= q0_split[1])\n",
    "    ccw_mask = ((points[:,1] >= points[:,0]) & \n",
    "                  (points[:,1] <= points[:,0] + torch.pi))\n",
    "    \n",
    "    # Mask to retrieve only the clockwise points\n",
    "    cw_mask = ((points[:,1] >= points[:,0] - torch.pi) & (points[:,1] <= points[:,0]))\n",
    "\n",
    "    if clockwise:\n",
    "        final_mask = width_mask & cw_mask\n",
    "    else:\n",
    "        final_mask = width_mask & ccw_mask\n",
    "    \n",
    "    points = points[final_mask]\n",
    "    points = points[0:3000]\n",
    "\n",
    "    if points.size(0) < 3000:\n",
    "        print(\"Warning: Only\", points.size(0), \"points in dataset.\")\n",
    "\n",
    "    return(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plotter(points, extend = None, save = False, file_path = None):\n",
    "\n",
    "   \"\"\" \n",
    "   Simple plotter function which visualizes the points used for training of the Autoencoder. \n",
    "   \"\"\"\n",
    "    \n",
    "   plt.figure(figsize=(6, 6))\n",
    "   plt.scatter(points[:, 0].cpu().numpy(), points[:, 1].cpu().numpy(), alpha=0.6, edgecolors='k', s=20)\n",
    "   plt.title('Scatter Plot of q0 vs q1')\n",
    "   plt.xlabel('q0')\n",
    "   plt.ylabel('q1')\n",
    "   plt.xlim(-2*torch.pi, 2*torch.pi)\n",
    "   plt.ylim(-2*torch.pi, 2*torch.pi)\n",
    "   plt.grid(True)\n",
    "   \n",
    "   if save:\n",
    "        plt.savefig(file_path, dpi=800)\n",
    "   plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_points = mask_points((-torch.pi, torch.pi), clockwise = train_clockwise)\n",
    "deshifted_points = transforms.wrap_to_pi(masked_points.clone())\n",
    "points_plotter(masked_points, extend=\"ccw\")\n",
    "points_plotter(deshifted_points, extend=\"ccw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_dataset(points):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute mass- and input matrix of all training points to reduce load in training.\n",
    "    Returns TensorDataset of (q, M_q, A_q). \n",
    "    \"\"\"\n",
    "\n",
    "    data_pairs = []\n",
    "    for point in points:\n",
    "        Mq_point, _, _ = dynamics.dynamical_matrices(rp, point, point)\n",
    "        Aq_point = dynamics.input_matrix(rp, point)\n",
    "        data_pairs.append((point, Mq_point, Aq_point))\n",
    "\n",
    "    points_tensor = torch.stack([pair[0] for pair in data_pairs])           # Tensor of all points\n",
    "    mass_matrices_tensor = torch.stack([pair[1] for pair in data_pairs])   # Tensor of all mass matrices\n",
    "    input_matrices_tensor = torch.stack([pair[2] for pair in data_pairs])  # Tensor of all input matrices\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(points_tensor, mass_matrices_tensor, input_matrices_tensor)\n",
    "    return(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(dataset, batch_size = 512, train_part = 0.7):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a training and validation dataloader from an input dataset, based on \n",
    "    batch size and the ratio train_part. \n",
    "    \"\"\"\n",
    "\n",
    "    train_size = int(train_part * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # Create TensorDataset for both training and testing sets\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create the DataLoader for both training and testing sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "f_recon        = 10.\n",
    "f_diag         = 1. #TODO: CHECK OUT THESE LOSS WEIGHTS\n",
    "f_off_diag     = 1. #TODO: CHECK OUT THESE LOSS WEIGHTS\n",
    "f_input        = 1.\n",
    "f_input_jac    = 1.\n",
    "f_theta        = 1.\n",
    "f_inertia_eye  = 0. #TODO: CHECK OUT THESE LOSS WEIGHTS\n",
    "\n",
    "\n",
    "def loss_fun(q, theta, q_hat, M_q, A_q, J_h_enc, J_h_dec):\n",
    "\n",
    "    \"\"\"\n",
    "    Loss function for training the Autoencoder. Loss terms are the following:\n",
    "    l_recon:    Loss between input- and reconstructed variable. (MSE)\n",
    "    l_off_dia:  Loss of off-diagonal terms of mass matrix in theta-space. (MSE)\n",
    "    l_dia:      Loss on diagonal terms of mass matrix in theta-space. \n",
    "                (mean of normalized negative log-loss)\n",
    "    l_input:    Loss to drive input matrix in theta-space to [1, 0]^T (MSE)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    l_recon = F.mse_loss(q, q_hat, reduction=\"mean\")\n",
    "\n",
    "    # Calculate forward and inverse Jacobians\n",
    "    J_h = J_h_enc\n",
    "    J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "    J_h_inv = J_h_dec\n",
    "    J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "    M_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "    A_th = J_h_inv_trans @ A_q\n",
    "\n",
    "    # Loss inspired by Pietro Pustina's paper on input decoupling:\n",
    "    # https://arxiv.org/pdf/2306.07258\n",
    "    l_input_jac = F.mse_loss(J_h[:, 0, :], A_q[:, :, 0], reduction=\"mean\")\n",
    "\n",
    "\n",
    "\n",
    "    # Loss on the first coordinate theta, again from Pietro Pustina's analytic formulation\n",
    "    theta_ana = torch.vmap(transforms.analytic_theta, in_dims = (None, 0))(rp, q)\n",
    "    l_theta = F.mse_loss(theta[:, 0], theta_ana[:, 0], reduction=\"mean\")\n",
    "\n",
    "    # Enforce inertial decoupling\n",
    "    l_off_diag = torch.mean((M_th[:, 0, 1])**2)\n",
    "    diag_values = torch.diagonal(M_th, dim1=1, dim2=2)\n",
    "    l_diag = torch.mean((-1 + torch.exp(-(diag_values - 1))) * (diag_values < 1).float())  # Shape: (batch_size, 2)\n",
    "\n",
    "    ## input decoupling loss\n",
    "    #l_input = torch.mean((A_th[:, 1]**2)) + torch.mean(((A_th[:, 0]-1)**2))\n",
    "    l_input = torch.mean((A_th[:, 1]**2))\n",
    "    \n",
    "    #print(torch.eye(M_th.size(-1)))]).\n",
    "    M_eye = torch.eye(M_th.size(-1), device=M_th.device, dtype=M_th.dtype).expand(M_th.size())\n",
    "    l_inertia_eye = F.mse_loss(M_th, M_eye, reduction=\"mean\")\n",
    "    \n",
    "    #loss_terms = torch.tensor([l_recon * f_recon, l_diag * f_diag, l_off_diag * f_off_diag, \n",
    "    #                           l_input * f_input, l_input_jac * f_input_jac, l_theta * f_theta])\n",
    "    #loss_sum = l_recon * f_recon + l_diag * f_diag + l_off_diag * f_off_diag + l_input * f_input + l_input_jac * f_input_jac + l_theta * f_theta\n",
    "\n",
    "    loss_terms = torch.tensor([l_recon * f_recon, l_inertia_eye * f_inertia_eye, \n",
    "                               l_input * f_input, l_input_jac * f_input_jac, l_theta * f_theta])\n",
    "    loss_sum = l_recon * f_recon + l_inertia_eye * f_inertia_eye + l_input * f_input + l_input_jac * f_input_jac + l_theta * f_theta\n",
    "\n",
    "    return loss_sum, loss_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.load_state_dict(torch.load(load_path, weights_only=True))\n",
    "\n",
    "\n",
    "def train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, val_dataloader, current_time, dir_name = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Executes training loop for Autoencoder\n",
    "    \"\"\"\n",
    "    if dir_name is None:\n",
    "        model = autoencoders.Autoencoder_double(rp).to(device)  # Move model to GPU\n",
    "        dir_name = f\"NN_{current_time}\"\n",
    "        file_counter = 0\n",
    "    else:\n",
    "        dir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "        nn_filename = dir_name + \"_0.pth\"\n",
    "        nn_filepath = os.path.join(dir_path, nn_filename)\n",
    "        file_counter = 0\n",
    "        while os.path.isfile(nn_filepath):\n",
    "            print(\"Looking for latest weights and bias file.\")\n",
    "            file_counter += 1\n",
    "            nn_filename = nn_filename[:-6] + \"_\" + str(file_counter) + \".pth\"\n",
    "            nn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "        model_path = os.path.join(\"Models\", dir_name, dir_name + \"_\" + str(file_counter-1) + \".pth\")\n",
    "        model = autoencoders.Autoencoder_double(rp).to(device)  # Initialize model architecture\n",
    "        model.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))  # Load weights\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#,  weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5 ** (1 / num_epochs))\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    start_time = time.time()       \n",
    "    dir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    nn_filename = dir_name + \"_\" + str(file_counter) + \".pth\"\n",
    "    nn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "    metadata = {\"q0_low\" : q0_split[0],\n",
    "                \"q0_high\" : q0_split[1],\n",
    "                \"lr\" : lr,\n",
    "                \"epochs\" : num_epochs,\n",
    "                \"file_name\" : nn_filename,\n",
    "                \"loss_weights\": {\n",
    "                    \"f_recon\": f_recon,\n",
    "                    \"f_diag\": f_diag,\n",
    "                    \"f_off_diag\": f_off_diag,\n",
    "                    \"f_input\": f_input,\n",
    "                    \"f_input_jac\": f_input_jac,\n",
    "                    \"f_theta\": f_theta,\n",
    "                    \"f_inertia_eye\": f_inertia_eye\n",
    "                }}\n",
    "    \n",
    "    metadata_filename = \"metadata_\" + str(file_counter) + \".json\"\n",
    "    metadata_filepath = os.path.join(dir_path, metadata_filename)\n",
    "    with open(metadata_filepath, \"w\") as f:\n",
    "        json.dump(metadata, f, indent=4)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_loss_terms = torch.zeros(5)\n",
    "        for index, (q, M_q, A_q) in enumerate(train_dataloader):\n",
    "            q = q.to(device)\n",
    "            M_q = M_q.to(device)\n",
    "            A_q = A_q.to(device)\n",
    "\n",
    "            batch_size = q.size(0)\n",
    "            \n",
    "            theta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)  \n",
    "            theta_ana = model.theta_ana(q)\n",
    "                    \n",
    "            loss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * batch_size\n",
    "            train_loss_terms += loss_terms * batch_size\n",
    "        train_loss /= len(train_dataloader.dataset) \n",
    "        train_loss_terms /= len(train_dataloader.dataset) \n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_terms = torch.zeros(5)\n",
    "        with torch.no_grad():\n",
    "            for index, (q, M_q, A_q) in enumerate(val_dataloader):\n",
    "                q = q.to(device)\n",
    "                M_q = M_q.to(device)\n",
    "                A_q = A_q.to(device)\n",
    "\n",
    "                batch_size = q.size(0)\n",
    "\n",
    "                theta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)\n",
    "                theta_ana = model.theta_ana(q)\n",
    "\n",
    "                loss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "\n",
    "                val_loss += loss.item() * batch_size\n",
    "                val_loss_terms += loss_terms * batch_size\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_loss_terms /= len(val_dataloader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        epoch_duration = time.time() - start_time\n",
    "        scheduler.step()\n",
    "        tlt = train_loss_terms\n",
    "        print(\n",
    "            f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Duration: {epoch_duration:.2f} seconds')\n",
    "        #print(\n",
    "        #    f'l_recon: {tlt[0]:.4f}, l_diag: {tlt[1]:.4f}, l_off_diag: {tlt[2]:.4f}, l_input: {tlt[3]:.4f}, l_input_jac: {tlt[4]:.4f}, l_theta: {tlt[5]:.4f}'\n",
    "        #)\n",
    "        print(\n",
    "            f'l_recon: {tlt[0]:.4f}, l_inertia: {tlt[1]:.4f}, l_input: {tlt[2]:.4f}, l_input_jac: {tlt[3]:.4f}, l_theta: {tlt[4]:.4f}'\n",
    "        )\n",
    "        \n",
    "    return(model, train_losses, val_losses, dir_path, nn_filepath, file_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f\"Model parameters saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, log = False, save = False, file_path = None):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots training and validation loss. \n",
    "    ylim\" and \"yscale\" should be enabled depending on the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.ylim((-1, 40))\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss over Epochs\")\n",
    "    plt.grid(True)\n",
    "    if log:\n",
    "        plt.yscale(\"log\")\n",
    "    if save:\n",
    "        plt.savefig(file_path, dpi=800)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d44a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plotting.plotters_simple as plotters_simple\n",
    "\n",
    "def make_plot_dataloader(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the training dataset and returns a dataloader of every 10th point\n",
    "    to reduce visual clutter. \n",
    "    \"\"\"\n",
    "\n",
    "    points_tensor, mass_matrices_tensor, input_matrices_tensor = dataset.tensors\n",
    "    \n",
    "    plot_sampled = points_tensor[::10]\n",
    "    mass_sampled = mass_matrices_tensor[::10]\n",
    "    input_sampled = input_matrices_tensor[::10]\n",
    "\n",
    "    plot_dataset = TensorDataset(plot_sampled, mass_sampled, input_sampled)\n",
    "    plot_dataloader = DataLoader(plot_dataset, batch_size=len(plot_dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "    return(plot_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c160a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(model, plot_dataloader, folder_path):\n",
    "\n",
    "    model_ana = autoencoders.Analytic_transformer(rp)\n",
    "\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (q, M_q, A_q) in plot_dataloader:\n",
    "            q = q.to(device)\n",
    "            M_q = M_q.to(device)\n",
    "            A_q = A_q.to(device)\n",
    "\n",
    "            theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)\n",
    "            theta_ana = model.theta_ana(q)\n",
    "            J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "            J_h_inv = J_h_dec\n",
    "            J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "            J_h_inv_ana = torch.linalg.inv(J_h_ana)\n",
    "            J_h_inv_trans_ana = torch.transpose(J_h_inv_ana, 1, 2)\n",
    "\n",
    "            M_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "            A_th = (J_h_inv_trans @ A_q).squeeze(-1)\n",
    "\n",
    "            M_th_ana = J_h_inv_trans_ana @ M_q @ J_h_inv_ana\n",
    "            A_th_ana = (J_h_inv_trans_ana @ A_q).squeeze(-1)\n",
    "\n",
    "            \n",
    "            plotters_simple.plot_3d_quad(q, [theta_ana[:, 0], theta[:, 0], theta_ana[:, 1], theta[:, 1]], \"Analytic vs learned theta\", \n",
    "                                         [\"th0_ana\", \"th0_learned\", \"th_1_ana\", \"th_1_learned\"], \"q_0\", \"q_1\", \"th\", folder_path)\n",
    "            \n",
    "\n",
    "\n",
    "            plotters_simple.plot_3d_double(q, [A_th[:, 0], A_th[:, 1]], \"Input decoupling\", [\"A0\", \"A1\"], \"q_0\", \"q_1\", \"A\", folder_path)\n",
    "            \n",
    "            A_th = A_th.cpu().detach().numpy()\n",
    "            print(\"Percentage of abs(A_0) > 0.6:\", 100 * np.sum(np.abs(A_th[:, 0]) > 0.6)/A_th[:, 0].size, \"%\")\n",
    "            print(\"Percentage of abs(A_1) < 0.3:\", 100 * np.sum(np.abs(A_th[:, 1]) < 0.3)/A_th[:, 1].size, \"%\")\n",
    "            \n",
    "            plotters_simple.plot_3d_quad(q, [M_th[:, 0, 0], M_th[:, 0, 1], M_th[:, 1, 0], M_th[:, 1, 1]], \"M_th vs q\", \n",
    "                                         [\"M_th[0,0]\", \"M_th[0,1]\", \"M_th[1,0]\", \"M_th[1,1]\"], \"q_0\", \"q_1\", \"M_th\", folder_path)\n",
    "            M_th_cpu = M_th.cpu().detach().numpy()\n",
    "            print(\"Percentage of abs(M_00) > 1.0:\", 100 * np.sum(np.abs(M_th_cpu[:, 0, 0]) > 1.0)/M_th_cpu[:, 0, 0].size, \"%\")\n",
    "            print(\"Percentage of abs(M_01) < 0.2:\", 100 * np.sum(np.abs(M_th_cpu[:, 0, 1]) < 0.2)/M_th_cpu[:, 0, 1].size, \"%\")\n",
    "            print(\"Percentage of abs(M_11) > 1.0:\", 100 * np.sum(np.abs(M_th_cpu[:, 1, 1]) > 1.0)/M_th_cpu[:, 1, 1].size, \"%\")\n",
    "\n",
    "            plotters_simple.plot_3d_quad(theta, [M_th[:, 0, 0], M_th[:, 0, 1], M_th[:, 1, 0], M_th[:, 1, 1]], \"M_th vs th\", \n",
    "                                         [\"M_th[0,0]\", \"M_th[0,1]\", \"M_th[1,0]\", \"M_th[1,1]\"], \"th_0\", \"th_1\", \"M_th\", folder_path)\n",
    "\n",
    "            plotters_simple.plot_3d_quad(theta, [M_th_ana[:, 0, 0], M_th_ana[:, 0, 1], M_th_ana[:, 1, 0], M_th_ana[:, 1, 1]], \"M_th_ana vs th\", \n",
    "                                         [\"M_th_ana[0,0]\", \"M_th_ana[0,1]\", \"M_th_ana[1,0]\", \"M_th_ana[1,1]\"], \"th_0\", \"th_1\", \"M_th_ana\", folder_path)\n",
    "\n",
    "\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 0, 0], J_h[:, 0, 1], \"J_h\", \"00\", \"01\", \"q_0\", \"q_1\", \"J_h\", device)\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 1, 0], J_h[:, 1, 1], \"J_h\", \"10\", \"11\", \"q_0\", \"q_1\", \"J_h\", device)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81624d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q0_split = (-torch.pi, torch.pi)\n",
    "batch_size = 512\n",
    "train_part = 0.7\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "num_epochs = 101\n",
    "lr = 1e-3\n",
    "\n",
    "# IMPORTANT:\n",
    "# If you want to train on an existing model\n",
    "existing_model_name = \"NN_202503231956\"\n",
    "existing_model_name = None\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "shifted_points = mask_points(q0_split, clockwise=train_clockwise)\n",
    "\n",
    "dataset = make_dataset(shifted_points)\n",
    "(train_dataloader, val_dataloader) = make_dataloaders(dataset=dataset, batch_size=batch_size, train_part=train_part)\n",
    "outputs = []\n",
    "model, train_losses, val_losses, dir_path, nn_filepath, file_counter = train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, \n",
    "                                                        val_dataloader, current_time, existing_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec749cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_path = os.path.join(dir_path, \"training_points_\" + str(file_counter) + \".png\")\n",
    "points_plotter(shifted_points, save = True, file_path = points_path)\n",
    "scatter_fig_path = os.path.join(dir_path, \"loss_\" + str(file_counter) + \".png\")\n",
    "plot_loss(train_losses, val_losses, log = True, save = True, file_path = scatter_fig_path)\n",
    "save_model(model, nn_filepath)\n",
    "print(nn_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_0_single(model, q):\n",
    "    print(\"q:\", q.size())\n",
    "    theta = model.encoder(q)\n",
    "    print(\"theta:\", theta.size())\n",
    "    return theta[:, 0].detach()\n",
    "    \n",
    "def theta_1_single(model, q):\n",
    "    theta = model.encoder(q)\n",
    "    return theta[:, 1].detach()\n",
    "    \n",
    "def q_hat_0_single(model, theta):\n",
    "    q_hat = model.decoder(theta)\n",
    "    return q_hat[:, 0].detach()\n",
    "    \n",
    "def q_hat_1_single(model, theta):\n",
    "    q_hat = model.decoder(theta)\n",
    "    return q_hat[:, 1].detach()\n",
    "\n",
    "mapping_functions = (partial(theta_0_single,model), \n",
    "                        partial(theta_1_single,model), \n",
    "                        partial(q_hat_0_single,model), \n",
    "                        partial(q_hat_1_single,model))\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, device=device, \n",
    "                                            mapping_functions=mapping_functions, mask_split=q0_split)\n",
    "theta_figure_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".png\")\n",
    "th_plotter.make_figure(theta_figure_path)\n",
    "theta_anim_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".mp4\")\n",
    "#th_plotter.make_animation(theta_anim_path, duration = 4, fps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "folder_path = os.path.join(dir_path, \"performance_plots_\" + str(file_counter))\n",
    "\n",
    "#q0_split = (-torch.pi, torch.pi)\n",
    "plt.ion()\n",
    "#model = autoencoders.Autoencoder_double(rp).to(device)\n",
    "#model_location = 'Models/Split_AEs/Lumped_Mass_202503051257.pth'\n",
    "#model.load_state_dict(torch.load(model_location, weights_only=True))\n",
    "plot_points = mask_points(q0_split, clockwise = train_clockwise)\n",
    "plot_dataset = make_dataset(plot_points)\n",
    "plot_dataloader = make_plot_dataloader(plot_dataset)\n",
    "plot_model_performance(model, plot_dataloader, folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e667f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "model_ana = autoencoders.Analytic_transformer(rp)\n",
    "\n",
    "\n",
    "models = [model_ana, model]\n",
    "model_names = [\"Analytic\", \"Neural Network\"]\n",
    "\n",
    "theta_xy_fig_path = os.path.join(dir_path, \"theta_vs_q\" + str(file_counter) + \".png\")\n",
    "\n",
    "def check_clockwise_vectorized(q):\n",
    "    \"\"\"\n",
    "    Expects q to be a tensor of shape (N,2) where each row is [q0, q1].\n",
    "    Returns two boolean masks: (cw_mask, ccw_mask), where:\n",
    "      - cw_mask[i] is True if the i-th configuration is elbow clockwise.\n",
    "      - ccw_mask[i] is True if the i-th configuration is elbow counterclockwise.\n",
    "    \n",
    "    The logic is as follows (from your original function):\n",
    "      If q1 lies between q0 and q0+π, or between q0-2π and q0-π, then the configuration\n",
    "      is considered counterclockwise. Otherwise it is clockwise.\n",
    "    \"\"\"\n",
    "    q0 = q[:, 0]\n",
    "    q1 = q[:, 1]\n",
    "    cond_ccw = ((q1 >= q0) & (q1 <= q0 + torch.pi))\n",
    "    cw_mask = ~cond_ccw\n",
    "    ccw_mask = cond_ccw\n",
    "    return cw_mask, ccw_mask\n",
    "\n",
    "\n",
    "# Define the number of grid points along each dimension.\n",
    "n_points = 200\n",
    "\n",
    "# Create 1D tensors for q0 and q1 in the range [-pi, 0]\n",
    "q0_vals = torch.linspace(-np.pi, np.pi, n_points)\n",
    "q1_vals = torch.linspace(-np.pi, 2*np.pi, n_points)\n",
    "\n",
    "# Create a 2D grid (meshgrid) of q values.\n",
    "# (Note: using indexing='ij' so that the first axis corresponds to q0 and the second to q1)\n",
    "q0_grid, q1_grid = torch.meshgrid(q0_vals, q1_vals, indexing='ij')\n",
    "\n",
    "# Stack the grid to get a tensor of shape (n_points*n_points, 2)\n",
    "q_grid = torch.stack([q0_grid.flatten(), q1_grid.flatten()], dim=1).to(device)\n",
    "\n",
    "# === Compute theta0 and theta1 using the analytic encoder functions ===\n",
    "# We use torch.vmap to evaluate the functions over the batch of q values.\n",
    "# Note: encoder_theta_0_ana and encoder_theta_1_ana each return a tuple (theta, theta).\n",
    "\n",
    "if train_clockwise:\n",
    "    raise ValueError(\"This plotter currently only supports plotting counterclockwise performance. Should be an easy fix\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 8.5))\n",
    "for i, (model, model_name) in enumerate(zip(models, model_names)):\n",
    "\n",
    "    theta_out = model.encoder_vmap(q_grid)\n",
    "    #theta_out = torch.vmap(model.encoder)(q_grid)\n",
    "\n",
    "    theta0 = theta_out[:, 0]\n",
    "    theta1 = theta_out[:, 1]\n",
    "\n",
    "    # Since q1_grid and q2_grid are already on a mesh, we can compute x_end and y_end elementwise.\n",
    "    x_end = rp[\"l0\"] * torch.cos(q_grid[:, 0]) + rp[\"l1\"] * torch.cos(q_grid[:, 1])\n",
    "    y_end = rp[\"l0\"] * torch.sin(q_grid[:, 0]) + rp[\"l1\"] * torch.sin(q_grid[:, 1])\n",
    "\n",
    "    # --- Determine configuration (clockwise vs. counterclockwise) for each q ---\n",
    "    cw_mask, ccw_mask = check_clockwise_vectorized(q_grid)\n",
    "    # Counterclockwise points\n",
    "    x_end_ccw   = x_end[ccw_mask].detach().cpu().numpy()\n",
    "    y_end_ccw   = y_end[ccw_mask].detach().cpu().numpy()\n",
    "    theta0_ccw  = theta0[ccw_mask].detach().cpu().numpy()\n",
    "    theta1_ccw  = theta1[ccw_mask].detach().cpu().numpy()\n",
    "    thetas = [theta0_ccw, theta1_ccw]\n",
    "\n",
    "\n",
    "    for j in range(2):\n",
    "        sc = axes[i, j].scatter(x_end_ccw, y_end_ccw, c=thetas[j], cmap='viridis', s=5)\n",
    "        axes[i, j].set_title(\"th_\" + str(j) + \" - \" + model_names[i])\n",
    "        axes[i, j].set_xlabel(\"x\")\n",
    "        axes[i, j].set_ylabel(\"y\")\n",
    "        plt.colorbar(sc, ax=axes[i, j])\n",
    "\n",
    "\n",
    "    #sc4 = axes[1].scatter(x_end_ccw, y_end_ccw, c=theta2_ccw, cmap='viridis', s=5)\n",
    "    #axes[1].set_title(\"Theta1 - Counterclockwise\")\n",
    "    #axes[1].set_xlabel(\"x\")\n",
    "    #axes[1].set_ylabel(\"y\")\n",
    "    #plt.colorbar(sc4, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(theta_xy_fig_path)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20799b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
