{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import Double_Pendulum.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.transforms as transforms\n",
    "import Double_Pendulum.dynamics as dynamics\n",
    "\n",
    "import training_data as training_data\n",
    "import Plotting.theta_visualiser as theta_visualizer\n",
    "import learning_plotters as lp\n",
    "\n",
    "import autoencoders\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rp = robot_parameters.LUMPED_PARAMETERS.copy()\n",
    "rp[\"m0\"] = 0.0\n",
    "print(rp)\n",
    "\n",
    "train_clockwise = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_points(q0_split, clockwise = False, q1_margin = 0.):\n",
    "\n",
    "\t\"\"\"\n",
    "\tReturns a set of [q0, q1] points based on \"q0_split\" limits on q0 and q1.\n",
    "\tThe limits on q1 depend on whether a clockwise or counterclockwise dataset is selected.\n",
    "\t\"\"\"   \n",
    "\n",
    "\t# Retrieve training points\n",
    "\tpoints = training_data.points.to(device)\n",
    "\t\n",
    "\t# Mask to retrieve only the counterclockwise points\n",
    "\twidth_mask = (points[:,0] >= q0_split[0]) & (points[:,0] <= q0_split[1])\n",
    "\tccw_mask = ((points[:,1] >= points[:,0] + q1_margin) & \n",
    "\t\t\t\t  (points[:,1] <= points[:,0] + torch.pi - q1_margin))\n",
    "\t\n",
    "\t# Mask to retrieve only the clockwise points\n",
    "\tcw_mask = ((points[:,1] >= points[:,0] - torch.pi + q1_margin) & (points[:,1] <= points[:,0] - q1_margin))\n",
    "\n",
    "\tif clockwise:\n",
    "\t\tfinal_mask = width_mask & cw_mask\n",
    "\telse:\n",
    "\t\tfinal_mask = width_mask & ccw_mask\n",
    "\t\n",
    "\tpoints = points[final_mask]\n",
    "\tpoints = points[0:6000]\n",
    "\n",
    "\tif points.size(0) < 6000:\n",
    "\t\tprint(\"Warning: Only\", points.size(0), \"points in dataset.\")\n",
    "\n",
    "\treturn(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc02d15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plotter(points, extend = None, save = False, file_path = None):\n",
    "\n",
    "\t\"\"\" \n",
    "\tSimple plotter function which visualizes the points used for training of the Autoencoder. \n",
    "\t\"\"\"\n",
    "\t \n",
    "\tplt.figure(figsize=(6, 6))\n",
    "\tplt.scatter(points[:, 0].cpu().detach().numpy(), points[:, 1].cpu().detach().numpy(), alpha=0.6, edgecolors='k', s=20)\n",
    "\tplt.title('Scatter Plot of q0 vs q1')\n",
    "\tplt.xlabel('q0')\n",
    "\tplt.ylabel('q1')\n",
    "\tplt.xlim(-2*torch.pi, 2*torch.pi)\n",
    "\tplt.ylim(-2*torch.pi, 2*torch.pi)\n",
    "\tplt.grid(True)\n",
    "\t\n",
    "\tif save:\n",
    "\t\tplt.savefig(file_path, dpi=800)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_points = mask_points((-torch.pi, torch.pi), clockwise = train_clockwise, q1_margin = 0.2)\n",
    "deshifted_points = transforms.wrap_to_pi(masked_points.clone())\n",
    "points_plotter(masked_points, extend=\"ccw\")\n",
    "points_plotter(deshifted_points, extend=\"ccw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(points):\n",
    "\n",
    "\t\"\"\"\n",
    "\tCompute mass- and input matrix of all training points to reduce load in training.\n",
    "\tReturns TensorDataset of (q, M_q, A_q). \n",
    "\t\"\"\"\n",
    "\n",
    "\tdata_pairs = []\n",
    "\tfor point in points:\n",
    "\t\tMq_point, _, _ = dynamics.dynamical_matrices(rp, point, point)\n",
    "\t\tAq_point = dynamics.input_matrix(rp, point)\n",
    "\t\tdata_pairs.append((point, Mq_point, Aq_point))\n",
    "\n",
    "\tpoints_tensor = torch.stack([pair[0] for pair in data_pairs])           # Tensor of all points\n",
    "\tmass_matrices_tensor = torch.stack([pair[1] for pair in data_pairs])   # Tensor of all mass matrices\n",
    "\tinput_matrices_tensor = torch.stack([pair[2] for pair in data_pairs])  # Tensor of all input matrices\n",
    "\n",
    "\t# Create TensorDataset\n",
    "\tdataset = TensorDataset(points_tensor, mass_matrices_tensor, input_matrices_tensor)\n",
    "\treturn(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(dataset, batch_size = 512, train_part = 0.7):\n",
    "\n",
    "\t\"\"\"\n",
    "\tCreates a training and validation dataloader from an input dataset, based on \n",
    "\tbatch size and the ratio train_part. \n",
    "\t\"\"\"\n",
    "\n",
    "\ttrain_size = int(train_part * len(dataset))\n",
    "\tval_size = len(dataset) - train_size\n",
    "\n",
    "\t# Create TensorDataset for both training and testing sets\n",
    "\ttrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\t# Create the DataLoader for both training and testing sets\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\tval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\treturn(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def loss_fun(q, theta, q_hat, M_q, A_q, J_h_enc, J_h_dec, loss_weights):\n",
    "\n",
    "\t\"\"\"\n",
    "\tLoss function for training the Autoencoder. Loss terms are the following:\n",
    "\tl_recon:    Loss between input- and reconstructed variable. (MSE)\n",
    "\tl_off_dia:  Loss of off-diagonal terms of mass matrix in theta-space. (MSE)\n",
    "\tl_dia:      Loss on diagonal terms of mass matrix in theta-space. \n",
    "\t\t\t\t(mean of normalized negative log-loss)\n",
    "\tl_input:    Loss to drive input matrix in theta-space to [1, 0]^T (MSE)\n",
    "\n",
    "\t\"\"\"\n",
    "\n",
    "\n",
    "\tlw = loss_weights\n",
    "\t\n",
    "\tdelta = torch.atan2(torch.sin(q-q_hat), torch.cos(q-q_hat))\n",
    "\tl_recon = F.mse_loss(delta, torch.zeros_like(delta), reduction=\"mean\")\n",
    "\n",
    "\n",
    "\t# Calculate forward and inverse Jacobians\n",
    "\tJ_h = J_h_enc\n",
    "\tJ_h_trans = torch.transpose(J_h, 1, 2)\n",
    "\tJ_h_inv = J_h_dec\n",
    "\tJ_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "\tM_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "\tA_th = J_h_inv_trans @ A_q\n",
    "\n",
    "\t# Loss inspired by Pietro Pustina's paper on input decoupling:\n",
    "\t# https://arxiv.org/pdf/2306.07258\n",
    "\tl_input_jac = F.mse_loss(J_h[:, 0, :], A_q[:, :, 0], reduction=\"mean\")\n",
    "\n",
    "\tl_J_eye = F.mse_loss( torch.bmm(J_h, J_h_inv), \n",
    "\t\t\t\t\t\ttorch.eye(2,device=J_h.device).unsqueeze(0).repeat(J_h.size(0),1,1),\n",
    "\t\t\t\t\t\treduction='mean' )\n",
    "\n",
    "\t# Loss on the first coordinate theta, again from Pietro Pustina's analytic formulation\n",
    "\ttheta_ana = torch.vmap(transforms.analytic_theta, in_dims = (None, 0))(rp, q)\n",
    "\tl_theta = F.mse_loss(theta[:, 0], theta_ana[:, 0], reduction=\"mean\")\n",
    "\n",
    "\t# Enforce inertial decoupling\n",
    "\tl_off_diag = torch.mean((M_th[:, 0, 1])**2)\n",
    "\tdiag_values = torch.diagonal(M_th, dim1=1, dim2=2)\n",
    "\tl_diag = torch.mean((-1 + torch.exp(-(diag_values - 1))) * (diag_values < 1).float())  # Shape: (batch_size, 2)\n",
    "\n",
    "\t## input decoupling loss\n",
    "\t#l_input = torch.mean((A_th[:, 1]**2)) + torch.mean(((A_th[:, 0]-1)**2))\n",
    "\tl_input = torch.mean((A_th[:, 1]**2))\n",
    "\t\n",
    "\t\n",
    "\tloss_terms = torch.tensor([l_recon * lw[\"w_recon\"], l_diag * lw[\"w_diag\"], l_off_diag * lw[\"w_off_diag\"], \n",
    "\t\t\t\t\t\t\t   l_input * lw[\"w_input\"], l_input_jac * lw[\"w_input_jac\"], l_theta * lw[\"w_theta\"], l_J_eye * lw[\"w_J_eye\"]])\n",
    "\tloss_sum = l_recon * lw[\"w_recon\"] + l_diag * lw[\"w_diag\"] + l_off_diag * lw[\"w_off_diag\"] + l_input * lw[\"w_input\"] + \\\n",
    "\t\t\t   l_input_jac * lw[\"w_input_jac\"] + l_theta * lw[\"w_theta\"] + l_J_eye * lw[\"w_J_eye\"]\n",
    "\n",
    "\treturn loss_sum, loss_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.load_state_dict(torch.load(load_path, weights_only=True))\n",
    "\n",
    "def load_model(current_time, num_epochs, lr, dir_name = None):\n",
    "\tif dir_name is None:\n",
    "\t\tmodel = autoencoders.Autoencoder_double(rp).to(device)  # Move model to GPU\n",
    "\t\tdir_name = f\"NN_{current_time}\"\n",
    "\t\tfile_counter = 0\n",
    "\telse:\n",
    "\t\tdir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "\t\tnn_filename = dir_name + \"_0.pth\"\n",
    "\t\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\t\tfile_counter = 0\n",
    "\t\twhile os.path.isfile(nn_filepath):\n",
    "\t\t\tprint(\"Looking for latest weights and bias file.\")\n",
    "\t\t\tfile_counter += 1\n",
    "\t\t\tnn_filename = nn_filename[:-6] + \"_\" + str(file_counter) + \".pth\"\n",
    "\t\t\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "\t\tmodel_path = os.path.join(\"Models\", dir_name, dir_name + \"_\" + str(file_counter-1) + \".pth\")\n",
    "\t\tmodel = autoencoders.Autoencoder_double(rp).to(device)  # Initialize model architecture\n",
    "\t\tmodel.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))  # Load weights\n",
    "\n",
    "\treturn model, file_counter, dir_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5646336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_metadata(q0_split, lr, num_epochs, nn_filename, model, file_counter, dir_path, loss_weights):\n",
    "\t\n",
    "\tmetadata = {\"q0_low\" : q0_split[0],\n",
    "\t\t\t\t\"q0_high\" : q0_split[1],\n",
    "\t\t\t\t\"lr\" : lr,\n",
    "\t\t\t\t\"epochs\" : num_epochs,\n",
    "\t\t\t\t\"file_name\" : nn_filename,\n",
    "\t\t\t\t\"loss_weights\": loss_weights\n",
    "\t\t\t\t}\n",
    "\tmetadata[\"architecture\"] = {\n",
    "\t\t\"encoder\": autoencoders.summarize_sequential(model.enc),\n",
    "\t\t\"decoder\": autoencoders.summarize_sequential(model.dec)\n",
    "\t}\n",
    "\n",
    "\tmetadata_filename = \"metadata_\" + str(file_counter) + \".json\"\n",
    "\tmetadata_filepath = os.path.join(dir_path, metadata_filename)\n",
    "\twith open(metadata_filepath, \"w\") as f:\n",
    "\t\tjson.dump(metadata, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aa3d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, val_dataloader, current_time, \n",
    "\t\t\t\t   loss_weights, load_dir_name = None, save_dir_name = None):\n",
    "\n",
    "\t\"\"\"\n",
    "\tExecutes training loop for autoencoder\n",
    "\t\"\"\"\n",
    "\n",
    "\tmodel, file_counter, alt_dir_name = load_model(current_time, num_epochs, lr, load_dir_name)\n",
    "\n",
    "\tif save_dir_name is None:\n",
    "\t\tsave_dir_name = alt_dir_name\n",
    "\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=lr)#,  weight_decay=1e-6)\n",
    "\tscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5 ** (1 / num_epochs))\n",
    "\t \n",
    "\tdir_path = os.path.join(os.getcwd(), \"Models\", save_dir_name)\n",
    "\tos.makedirs(dir_path, exist_ok=True)\n",
    "\tnn_filename = \"NN_\" + str(file_counter) + \".pth\"\n",
    "\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "\n",
    "\n",
    "\tmake_metadata(q0_split, lr, num_epochs, nn_filename, model, file_counter, dir_path, loss_weights)\n",
    "\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\tstart_time = time.time()  \n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\n",
    "\t\t# Training phase\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss = 0\n",
    "\t\ttrain_loss_terms = torch.zeros(7)\n",
    "\t\tfor index, (q, M_q, A_q) in enumerate(train_dataloader):\n",
    "\t\t\tq = q.to(device)\n",
    "\t\t\tM_q = M_q.to(device)\n",
    "\t\t\tA_q = A_q.to(device)\n",
    "\n",
    "\t\t\tbatch_size = q.size(0)\n",
    "\t\t\t\n",
    "\t\t\ttheta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)  \n",
    "\t\t\ttheta_ana = model.theta_ana(q)\n",
    "\t\t\t\t\t\n",
    "\t\t\tloss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec, loss_weights)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\ttrain_loss += loss.item() * batch_size\n",
    "\t\t\ttrain_loss_terms += loss_terms * batch_size\n",
    "\t\ttrain_loss /= len(train_dataloader.dataset) \n",
    "\t\ttrain_loss_terms /= len(train_dataloader.dataset) \n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\n",
    "\t\t# Validation phase\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\tval_loss_terms = torch.zeros(7)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor index, (q, M_q, A_q) in enumerate(val_dataloader):\n",
    "\t\t\t\tq = q.to(device)\n",
    "\t\t\t\tM_q = M_q.to(device)\n",
    "\t\t\t\tA_q = A_q.to(device)\n",
    "\n",
    "\t\t\t\tbatch_size = q.size(0)\n",
    "\n",
    "\t\t\t\ttheta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)\n",
    "\t\t\t\ttheta_ana = model.theta_ana(q)\n",
    "\n",
    "\t\t\t\tloss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec, loss_weights)\n",
    "\n",
    "\t\t\t\tval_loss += loss.item() * batch_size\n",
    "\t\t\t\tval_loss_terms += loss_terms * batch_size\n",
    "\t\tval_loss /= len(val_dataloader.dataset)\n",
    "\t\tval_loss_terms /= len(val_dataloader.dataset)\n",
    "\t\tval_losses.append(val_loss)\n",
    "\t\tepoch_duration = time.time() - start_time\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\n",
    "\t\t\n",
    "\t\ttlt = train_loss_terms\n",
    "\t\tif epoch % 50 == 0:\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Duration: {epoch_duration:.2f} seconds')\n",
    "\t\t\tprint(\n",
    "\t\t\t\tf'l_recon: {tlt[0]:.4f}, l_diag: {tlt[1]:.4f}, l_off_diag: {tlt[2]:.4f}, l_input: {tlt[3]:.4f}, l_input_jac: {tlt[4]:.4f}, l_theta: {tlt[5]:.4f}, l_J_eye: {tlt[6]:.4f}'\n",
    "\t\t)\n",
    "\t\t\n",
    "\treturn(model, train_losses, val_losses, dir_path, nn_filepath, file_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9817f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "\ttorch.save(model.state_dict(), file_path)\n",
    "\tprint(f\"Model parameters saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, log = False, save = False, file_path = None):\n",
    "\n",
    "\t\"\"\"\n",
    "\tPlots training and validation loss. \n",
    "\tylim\" and \"yscale\" should be enabled depending on the loss function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_losses, label=\"Training Loss\")\n",
    "\tplt.plot(val_losses, label=\"Validation Loss\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\t#plt.ylim((-1, 40))\n",
    "\tplt.legend()\n",
    "\tplt.title(\"Training and Validation Loss over Epochs\")\n",
    "\tplt.grid(True)\n",
    "\tif log:\n",
    "\t\tplt.yscale(\"log\")\n",
    "\tif save:\n",
    "\t\tplt.savefig(file_path, dpi=800)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97acc686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_loss(train_losses, val_losses, save_loss_path):\n",
    "\tlosses_dict = {\n",
    "\t\t\"train\": train_losses,\n",
    "\t\t\"val\": val_losses\n",
    "\t}\n",
    "\t\n",
    "\ttorch.save(losses_dict, save_loss_path)\n",
    "\tprint(f\"Saved losses to {save_loss_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c66e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_loss(load_loss_path):\n",
    "\tlosses_dict = torch.load(load_loss_path, map_location = \"cpu\")\n",
    "\treturn losses_dict[\"train\"], losses_dict[\"val\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d44a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plotting.plotters_simple as plotters_simple\n",
    "\n",
    "def make_plot_dataloader(dataset, stride = 1):\n",
    "\n",
    "\t\"\"\"\n",
    "\tTakes the training dataset and returns a dataloader of every 10th point\n",
    "\tto reduce visual clutter. \n",
    "\t\"\"\"\n",
    "\n",
    "\tpoints_tensor, mass_matrices_tensor, input_matrices_tensor = dataset.tensors\n",
    "\t\n",
    "\tplot_sampled = points_tensor[::stride]\n",
    "\tmass_sampled = mass_matrices_tensor[::stride]\n",
    "\tinput_sampled = input_matrices_tensor[::stride]\n",
    "\n",
    "\tplot_dataset = TensorDataset(plot_sampled, mass_sampled, input_sampled)\n",
    "\tplot_dataloader = DataLoader(plot_dataset, batch_size=len(plot_dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "\treturn(plot_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c160a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_recon        = 4.0\n",
    "w_J_eye \t   = 1.0\n",
    "\n",
    "w_diag         = 1.25\n",
    "w_off_diag     = 1.25\n",
    "\n",
    "w_input        = 1.\n",
    "w_input_jac    = 0.8\n",
    "w_theta        = 1.\n",
    "\n",
    "loss_weights = {\n",
    "\t\"w_recon\": w_recon,\n",
    "\t\"w_J_eye\": w_J_eye,\n",
    "\t\"w_diag\": w_diag,\n",
    "\t\"w_off_diag\": w_off_diag,\n",
    "\t\"w_input\": w_input,\n",
    "\t\"w_input_jac\": w_input_jac,\n",
    "\t\"w_theta\": w_theta\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81624d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q0_split = (-torch.pi, torch.pi)\n",
    "batch_size = 512\n",
    "train_part = 0.7\n",
    "\n",
    "num_epochs = 3001\n",
    "lr = 1e-3\n",
    "\n",
    "# IMPORTANT:\n",
    "# If you want to train on an existing model\n",
    "#existing_model_name = \"NN_202505131531\"\n",
    "existing_model_name = None\n",
    "save_dir_name = None\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "training_points = mask_points(q0_split, clockwise=train_clockwise, q1_margin = 0.2)\n",
    "points_plotter(training_points)\n",
    "dataset = make_dataset(training_points)\n",
    "(train_dataloader, val_dataloader) = make_dataloaders(dataset=dataset, batch_size=batch_size, train_part=train_part)\n",
    "outputs = []\n",
    "model, train_losses, val_losses, dir_path, nn_filepath, file_counter = train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tval_dataloader, current_time, loss_weights, existing_model_name, save_dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa49c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, nn_filepath)\n",
    "save_loss_path = dir_path + \"/losses.pt\"\n",
    "save_loss(train_losses, val_losses, save_loss_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec749cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_path = os.path.join(dir_path, \"training_points_\" + str(file_counter) + \".png\")\n",
    "points_plotter(training_points, save = True, file_path = points_path)\n",
    "lp.plot_loss(train_losses, val_losses, file_counter, log = True,  save_folder = dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_0_single(model, q):\n",
    "\tprint(\"q:\", q.size())\n",
    "\ttheta = model.encoder(q)\n",
    "\tprint(\"theta:\", theta.size())\n",
    "\treturn theta[:, 0].detach()\n",
    "\t\n",
    "def theta_1_single(model, q):\n",
    "\ttheta = model.encoder(q)\n",
    "\treturn theta[:, 1].detach()\n",
    "\t\n",
    "def q_hat_0_single(model, theta):\n",
    "\tq_hat = model.decoder(theta)\n",
    "\treturn q_hat[:, 0].detach()\n",
    "\t\n",
    "def q_hat_1_single(model, theta):\n",
    "\tq_hat = model.decoder(theta)\n",
    "\treturn q_hat[:, 1].detach()\n",
    "\n",
    "mapping_functions = (partial(theta_0_single,model), \n",
    "\t\t\t\t\t\tpartial(theta_1_single,model), \n",
    "\t\t\t\t\t\tpartial(q_hat_0_single,model), \n",
    "\t\t\t\t\t\tpartial(q_hat_1_single,model))\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, device=device, \n",
    "\t\t\t\t\t\t\t\t\t\t\tmapping_functions=mapping_functions, mask_split=q0_split)\n",
    "theta_figure_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".png\")\n",
    "th_plotter.make_figure(theta_figure_path)\n",
    "theta_anim_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".mp4\")\n",
    "#th_plotter.make_animation(theta_anim_path, duration = 4, fps = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dbfeb1",
   "metadata": {},
   "source": [
    "#### Learned coordinate performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618954be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_model_performance(model, clockwise, q1_margin):\n",
    "\n",
    "\ttest_points = mask_points(q0_split, clockwise = clockwise, q1_margin = q1_margin)\n",
    "\ttest_dataset = make_dataset(test_points)\n",
    "\ttest_dataloader = make_plot_dataloader(test_dataset, stride=10)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\tfor (q_test, M_q_test, A_q_test) in test_dataloader:\n",
    "\t\t\tq_test = q_test.to(device)\n",
    "\t\t\tM_q_test = M_q_test.to(device)\n",
    "\t\t\tA_q_test = A_q_test.to(device)\n",
    "\n",
    "\t\t\ttheta_test, J_h_test, q_hat_test, J_h_dec_test, J_h_ana_test = model(q_test)\n",
    "\t\t\ttheta_ana_test = model.theta_ana(q_test)\n",
    "\t\t\tJ_h_trans_test = torch.transpose(J_h_test, 1, 2)\n",
    "\t\t\tJ_h_inv_test = J_h_dec_test\n",
    "\t\t\tJ_h_inv_trans_test = torch.transpose(J_h_inv_test, 1, 2)\n",
    "\n",
    "\t\t\tJ_h_inv_ana_test = torch.linalg.inv(J_h_ana_test)\n",
    "\t\t\tJ_h_inv_trans_ana_test = torch.transpose(J_h_inv_ana_test, 1, 2)\n",
    "\n",
    "\t\t\tM_th_test = J_h_inv_trans_test @ M_q_test @ J_h_inv_test\n",
    "\t\t\tA_th_test = (J_h_inv_trans_test @ A_q_test).squeeze(-1)\n",
    "\n",
    "\t\t\tM_th_ana_test = J_h_inv_trans_ana_test @ M_q_test @ J_h_inv_ana_test\n",
    "\t\t\tA_th_ana_test = (J_h_inv_trans_ana_test @ A_q_test).squeeze(-1)\n",
    "\n",
    "\tmse_A_th = F.mse_loss(A_th_test, A_th_ana_test)\n",
    "\tdiag_values_test = torch.diagonal(M_th_test, dim1=1, dim2=2)\n",
    "\tmse_M_th_dia = torch.mean((-1 + torch.exp(-(diag_values_test - 1))) * (diag_values_test < 1).float())\n",
    "\tmse_M_th_off_dia = F.mse_loss(M_th_test[:, 0, 1], torch.tensor(0.).repeat(M_th_test.size(0)).to(device))\n",
    "\tmse_M_th = mse_M_th_dia + mse_M_th_off_dia\n",
    "\tmse_recon = F.mse_loss(q_hat_test, q_test)\n",
    "\tmse_J_eye = F.mse_loss(J_h_test @ J_h_dec_test, torch.eye(2).repeat(J_h_test.size(0), 1, 1).to(device))\n",
    "\tprint(\"MSE A_th  loss:\", mse_A_th.item())\n",
    "\tprint(\"MSE M_th  loss:\", mse_M_th.item())\n",
    "\tprint(\"MSE recon loss:\", mse_recon.item())\n",
    "\tprint(\"MSE J_eye loss:\", mse_J_eye.item())\n",
    "\tmse_total = mse_A_th + mse_M_th + mse_recon + mse_J_eye\n",
    "\tprint(\"MSE total loss:\", mse_total.item())\n",
    "\n",
    "\treturn mse_A_th, mse_M_th_dia, mse_M_th_off_dia, mse_recon, mse_J_eye, mse_total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8e2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_loss_weights = {\n",
    "'w_recon': 4., \n",
    "'w_J_eye': 1., \n",
    "'w_diag': 1.25, \n",
    "'w_off_diag': 1.25, \n",
    "'w_input': 1., \n",
    "'w_input_jac': 0.8, \n",
    "'w_theta': 1.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab765424",
   "metadata": {},
   "outputs": [],
   "source": [
    "q0_split = (-torch.pi, 0.)\n",
    "batch_size = 512\n",
    "train_part = 0.7\n",
    "\n",
    "num_epochs = 3001\n",
    "lr = 1e-3\n",
    "\n",
    "existing_model_name = None\n",
    "\n",
    "shifted_points = mask_points(q0_split, clockwise=train_clockwise)\n",
    "\n",
    "dataset = make_dataset(shifted_points)\n",
    "train_dataloader, val_dataloader = make_dataloaders(dataset=dataset, batch_size=batch_size, train_part=train_part)\n",
    "\n",
    "mse_file_path = \"Models/Weight_Comparisons/Model_losses.json\"\n",
    "\n",
    "# Step 1: Load existing data if the file exists\n",
    "if os.path.exists(mse_file_path):\n",
    "    with open(mse_file_path, \"r\") as f:\n",
    "        mse_dict = json.load(f)\n",
    "else:\n",
    "    mse_dict = {}\n",
    "\n",
    "for i in range(len(loss_weights)):\n",
    "\tfor j in [0.25, 1., 4.]:\n",
    "\t\tloss_weights_loop = loss_weights.copy()\n",
    "\t\tweights_key = list(loss_weights.keys())[i]\n",
    "\t\tloss_weights_loop[weights_key] = j\n",
    "\t\t\n",
    "\n",
    "\t\tplt.ion()\n",
    "\t\tcurrent_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\t\tsave_dir_name = \"Weight_Comparisons/NN_[\" + str(i) + \"] = \" + str(j)\n",
    "\t\tprint(save_dir_name)\n",
    "\n",
    "\t\tmodel, train_losses, val_losses, dir_path, nn_filepath, file_counter = train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tval_dataloader, current_time, loss_weights, existing_model_name, save_dir_name)\n",
    "\t\tsave_model(model, nn_filepath)\n",
    "\t\tmse_A_th, mse_M_th_dia, mse_M_th_off_dia, mse_recon, mse_J_eye, mse_total = calc_model_performance(model)\n",
    "\n",
    "\t\tmse_dict_small = {\n",
    "\t\t\t\"mse_A_th\": mse_A_th.item(),\n",
    "\t\t\t\"mse_M_th_dia\": mse_M_th_dia.item(),\n",
    "\t\t\t\"mse_M_th_off_dia\": mse_M_th_off_dia.item(),\n",
    "\t\t\t\"mse_recon\": mse_recon.item(),\n",
    "\t\t\t\"mse_J_eye\": mse_J_eye.item(),\n",
    "\t\t\t\"mse_total\": mse_total.item()\n",
    "\t\t}\n",
    "\n",
    "\t\tmse_dict_big_name = \"loss item \" + str(i) + \", value \" + str(j)\n",
    "\t\tmse_dict[mse_dict_big_name] = mse_dict_small\n",
    "\n",
    "\t\twith open(mse_file_path, \"w\") as f:\n",
    "\t\t\tjson.dump(mse_dict, f, indent=4)\n",
    "\n",
    "\t\tnotify()\n",
    "        os.system('paplay /usr/share/sounds/freedesktop/stereo/complete.oga')\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c74579",
   "metadata": {},
   "source": [
    "#### Learned coordinate performance plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "folder_path = os.path.join(dir_path, \"performance_plots_\" + str(file_counter))\n",
    "\n",
    "plt.ion()\n",
    "plot_points = mask_points(q0_split, clockwise = train_clockwise)\n",
    "plot_dataset = make_dataset(plot_points)\n",
    "plot_dataloader = make_plot_dataloader(plot_dataset, stride = 1)\n",
    "lp.plot_model_performance(model, plot_dataloader, folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e667f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "plt.rcParams.update({\n",
    "\t'font.size':       16,   # base font size for text\n",
    "\t'axes.titlesize':  14,   # title of each subplot\n",
    "\t'axes.labelsize':  14,   # x/y axis labels\n",
    "\t'xtick.labelsize': 12,   # x‐tick labels\n",
    "\t'ytick.labelsize': 12,   # y‐tick labels\n",
    "\t'legend.fontsize': 14,   # legend text (if you had one)\n",
    "\t'figure.titlesize': 18,  # suptitle\n",
    "})\n",
    "\n",
    "\n",
    "try:\n",
    "\tdir_name\n",
    "except NameError:\n",
    "\tdir_name = \"NN_202505141642(half-q)\"\n",
    "\n",
    "\tdir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "\tnn_filename = \"NN_202505141642\" + \"_0.pth\"\n",
    "\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "\tmodel_path = nn_filepath\n",
    "\tmodel = autoencoders.Autoencoder_double(rp).to(device)  # Initialize model architecture\n",
    "\tmodel.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))  # Load weights\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9712dd2",
   "metadata": {},
   "source": [
    "#### Plot losses for mutiple simulation runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20799b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_paths = [\n",
    "\t\"Models/NN_202505221832/losses.pt\",\n",
    "\t\"Models/NN_202505221839/losses.pt\",\n",
    "\t\"Models/NN_202505221919/losses.pt\"\n",
    "]\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family']   = 'serif'\n",
    "matplotlib.rcParams['font.serif']    = ['Times New Roman']\n",
    "matplotlib.rcParams['mathtext.fontset'] = 'dejavuserif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a96d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses_vs_epoch(loss_paths):\n",
    "\t\"\"\"\n",
    "\tPlot training & validation losses over epochs for multiple runs.\n",
    "\t\n",
    "\t- Semi-transparent thin lines for each individual run\n",
    "\t- Bold line = mean across runs\n",
    "\t- Shaded band = ±1 standard deviation\n",
    "\t\"\"\"\n",
    "\tall_train = []\n",
    "\tall_val   = []\n",
    "\t\n",
    "\t# Load all losses\n",
    "\tfor loss_path in loss_paths:\n",
    "\t\ttrain_losses, val_losses = load_loss(loss_path)\n",
    "\t\tall_train.append(train_losses)\n",
    "\t\tall_val.append(val_losses)\n",
    "\t\n",
    "\t# Assume every run has the same number of epochs\n",
    "\tepochs = np.arange(0, len(all_train[0]))\n",
    "\tall_train = np.vstack(all_train)  # shape = (n_runs, n_epochs)\n",
    "\tall_val   = np.vstack(all_val)\n",
    "\t\n",
    "\t# Compute mean & std\n",
    "\tmean_train = all_train.mean(axis=0)\n",
    "\tstd_train  = all_train.std(axis=0)\n",
    "\tmean_val   = all_val.mean(axis=0)\n",
    "\tstd_val    = all_val.std(axis=0)\n",
    "\t\n",
    "\tplt.figure(figsize=(8, 6))\n",
    "\t\n",
    "\t# Plot each individual run (train & val) with low alpha\n",
    "\tfor i in range(all_train.shape[0]):\n",
    "\t\tplt.plot(epochs, all_train[i], color='C0', alpha=0.3, linewidth=1)\n",
    "\t\tplt.plot(epochs, all_val[i],   color='C1', alpha=0.3, linewidth=1, linestyle='--')\n",
    "\t\n",
    "\t# Plot mean ± std shading\n",
    "\tplt.plot(epochs, mean_train, color='C0', label='Train mean', linewidth=2)\n",
    "\tplt.fill_between(epochs,\n",
    "\t\t\t\t\t mean_train - std_train,\n",
    "\t\t\t\t\t mean_train + std_train,\n",
    "\t\t\t\t\t color='C0', alpha=0.2,\n",
    "\t\t\t\t\t label='Train ±1σ')\n",
    "\n",
    "\tplt.plot(epochs, mean_val, color='C1', label='Val mean', linewidth=2, linestyle='--')\n",
    "\tplt.fill_between(epochs,\n",
    "\t\t\t\t\t mean_val - std_val,\n",
    "\t\t\t\t\t mean_val + std_val,\n",
    "\t\t\t\t\t color='C1', alpha=0.2,\n",
    "\t\t\t\t\t label='Val ±1σ')\n",
    "\t\n",
    "\tplt.xlabel('Epoch')\n",
    "\tplt.ylabel('Loss')\n",
    "\tplt.title('Training & Validation Loss Across Runs')\n",
    "\tplt.grid(True)\n",
    "\tplt.legend()\n",
    "\tplt.yscale(\"log\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c33b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses_vs_epoch(loss_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844653fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
