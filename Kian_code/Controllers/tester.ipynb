{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "print(sys.path)\n",
    "\n",
    "import Learning.autoencoders as autoencoders\n",
    "import Double_Pendulum.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.dynamics as dynamics\n",
    "\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "rp[\"m1\"] = 0.\n",
    "model = autoencoders.Analytic_transformer(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_theta_1(rp: dict, q):\n",
    "    \n",
    "    # h1 is defined as the length between the actuator attachment point and the mass of the double pendulum\n",
    "    \n",
    "    Rx = rp[\"xa\"] - rp[\"l1\"] * torch.cos(q[0]) - rp[\"l2\"] * torch.cos(q[1])\n",
    "    Ry = rp[\"ya\"] - rp[\"l1\"] * torch.sin(q[0]) - rp[\"l2\"] * torch.sin(q[1])\n",
    "    \n",
    "    th1 = torch.sqrt(Rx**2 + Ry**2)\n",
    "    \n",
    "    return th1\n",
    "    \n",
    "    \n",
    "def analytic_theta_2(rp: dict, q):\n",
    "    \n",
    "    # h2 is defined as the arctan between the vector from mass of double pendulum \n",
    "    # to the actuator point. \n",
    "    # This was Cosimo's hunch, and has been verified in the Mathematica code\n",
    "    \n",
    "    Rx = rp[\"xa\"] - rp[\"l1\"] * torch.cos(q[0]) - rp[\"l2\"] * torch.cos(q[1])\n",
    "    Ry = rp[\"ya\"] - rp[\"l1\"] * torch.sin(q[0]) - rp[\"l2\"] * torch.sin(q[1])    \n",
    "    \n",
    "    th2 = torch.atan2(Ry,Rx)\n",
    "    \n",
    "    return th2\n",
    "\n",
    "def analytic_theta(rp:dict, q):\n",
    "    th1 = analytic_theta_1(rp, q)\n",
    "    th2 = analytic_theta_2(rp, q)\n",
    "\n",
    "    th = torch.stack([th1, th2], dim=-1)\n",
    "\n",
    "    return th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_inverse(rp: dict, th):\n",
    "\n",
    "    \"\"\"\n",
    "    Inverse kinematics from theta to q, based on the end-effector\n",
    "    position (xend, yend). \n",
    "    Returns a tuple with two sets of joint angles, one for clockwise\n",
    "    and one for counter-clockwise configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtain end effector position.\n",
    "\n",
    "    xend = rp[\"xa\"] - th[0]*torch.cos(th[1])\n",
    "    yend = rp[\"ya\"] - th[0]*torch.sin(th[1])\n",
    "\n",
    "    # Calculate the inside angle of the two joints, used to determine q1. Epsilon prevents NaN.\n",
    "    epsilon = 0.00001\n",
    "\n",
    "    numerator = (xend**2 + yend**2 - rp[\"l1\"]**2 - rp[\"l2\"]**2)\n",
    "    denominator = torch.tensor(2*rp[\"l1\"]*rp[\"l2\"])\n",
    "    fraction = numerator/denominator\n",
    "\n",
    "    \"\"\"\n",
    "    if torch.abs(fraction - 1) > 1.1:\n",
    "        raise ValueError(\"End effector outside of robot reach, inverse cannot be calculated\")\n",
    "    else:\n",
    "    \"\"\"\n",
    "    \n",
    "    epsilon = 1e-6\n",
    "    fraction = torch.clamp(fraction, -1.0 + epsilon, 1.0 - epsilon)\n",
    "\n",
    "    beta = torch.arccos(fraction)\n",
    "\n",
    "    # Determine primary angles.\n",
    "    q1 = torch.atan2(yend, xend + epsilon) - torch.atan2(rp[\"l2\"]*torch.sin(beta), epsilon + rp[\"l1\"] + rp[\"l2\"]*torch.cos(beta))\n",
    "    q2 = q1 + beta\n",
    "\n",
    "    # Determine secondary angles.\n",
    "    q1_alt = torch.atan2(yend, xend) + torch.atan2(rp[\"l2\"]*torch.sin(beta), epsilon + rp[\"l1\"] + rp[\"l2\"]*torch.cos(beta))\n",
    "    q2_alt = q1_alt - beta \n",
    "\n",
    "    # Normalize values between -pi and pi.\n",
    "    q1 = (q1 + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "    q2 = (q2 + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "    q1_alt = (q1_alt + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "    q2_alt = (q2_alt + torch.pi) % (2 * torch.pi) - torch.pi\n",
    "\n",
    "    q = torch.stack([q1, q2], dim=-1)\n",
    "    q_alt = torch.stack([q1_alt, q2_alt], dim=-1)\n",
    "\n",
    "    # Check whether the primary angle is clockwise. Otherwise, swap with secondary.\n",
    "    q_cw = q\n",
    "    q_ccw = q_alt\n",
    "    \n",
    "    return q_cw, q_ccw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = torch.tensor([[0.2, 0.3]]).requires_grad_(True)\n",
    "q_d = torch.tensor([[0.1, 0.4]]).requires_grad_(True)\n",
    "\n",
    "th = model.encoder(q.squeeze(0)).unsqueeze(0)\n",
    "#th = analytic_theta(rp, q)\n",
    "th_copy = th.detach().clone().requires_grad_(True)\n",
    "q_hat = model.decoder(th_copy, clockwise = False)\n",
    "\n",
    "J_forward = model.jacobian_enc(q, clockwise = False)\n",
    "J_inverse = model.jacobian_dec(th, clockwise = False)\n",
    "J_inverse_copy = model.jacobian_dec(th_copy, clockwise = False)\n",
    "J_inverse_copy_trans = torch.transpose(J_inverse_copy, 0, 1)\n",
    "\n",
    "th_d = (J_forward @ q_d.T).T\n",
    "th_d_copy = th_d.detach().clone().requires_grad_(True)\n",
    "q_d_hat = (J_inverse_copy @ th_d_copy.T).T\n",
    "\n",
    "print(torch.autograd.grad(q_hat[0, 0], th_copy, create_graph=True))\n",
    "\n",
    "\n",
    "print(\"th_copy gradient?\", th_copy.requires_grad)\n",
    "print(\"J_inv_copy gradient?\", J_inverse_copy.requires_grad)\n",
    "\n",
    "print(\"th_copy:\", th_copy)\n",
    "print(\"q size:\", q.size())\n",
    "print(\"q_hat size:\", q_hat.size())\n",
    "\n",
    "print(\"q_hat_squeeze size:\", q_hat.squeeze(0).size())\n",
    "Mq, Cq, Gq = dynamics.dynamical_matrices(rp, q_hat.squeeze(0), q_d_hat.squeeze(0))\n",
    "#print(torch.autograd.grad(Mq[0, 1], th_copy))\n",
    "print(\"Mq:\\n\", Mq)\n",
    "\n",
    "#print(torch.autograd.grad(J_inverse_copy_trans[1,1], th_copy))\n",
    "\n",
    "Mth = J_inverse_copy_trans @ Mq @ J_inverse_copy\n",
    "print(\"Mth\\n\", Mth)\n",
    "M1 = Mth[1,1]\n",
    "print(M1)\n",
    "dM00dth = torch.autograd.grad(Mth[0,0], th_copy, create_graph=True)[0]\n",
    "dM01dth = torch.autograd.grad(Mth[0,1], th_copy, create_graph=True)[0]\n",
    "dM10dth = torch.autograd.grad(Mth[1,0], th_copy, create_graph=True)[0]\n",
    "dM11dth = torch.autograd.grad(Mth[1,1], th_copy, create_graph=True)[0]\n",
    "print(dM00dth)\n",
    "print(dM01dth)\n",
    "print(dM10dth)\n",
    "print(dM11dth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rp[\"m2\"] * 2*th[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
