{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import functorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import Double_Pendulum.Lumped_Mass.robot_parameters as robot_parameters\n",
    "import Double_Pendulum.Lumped_Mass.transforms as transforms\n",
    "import Double_Pendulum.Lumped_Mass.dynamics as dynamics\n",
    "import Learning.loss_terms as loss_terms\n",
    "#import Plotting.plotters_h1h2 as plotters_h1h2\n",
    "import Learning.training_data as training_data\n",
    "import Plotting.theta_visualiser as theta_visualizer\n",
    "\n",
    "from Kian_code.Models.autoencoders import Autoencoder_double\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_points(q1_split, angle_margin = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a set of [q1, q2] points based on \"q1_split\" limits on q1 and an angle_margin on q2.\n",
    "    If angle_margin = 0, only returns points for which q1 < q2 < q1 + pi.\n",
    "    With angle_margin > 0, reduces points to prevent those close to non-singular mapping\n",
    "    \"\"\"   \n",
    "\n",
    "\n",
    "    # Retrieve training points\n",
    "    points = training_data.points.to(device)\n",
    "    \n",
    "    valid_mask = ((points[:,1] >= points[:,0] + angle_margin) & \n",
    "                  (points[:,1] <= points[:,0] + torch.pi -angle_margin) & \n",
    "                  (points[:,0] >= q1_split[0]) &\n",
    "                  (points[:,0] <= q1_split[1]))\n",
    "    \n",
    "    points = points[valid_mask]\n",
    "    points = points[0:3000]\n",
    "\n",
    "    if points.size(0) < 3000:\n",
    "        print(\"Warning: Only\", points.size(0), \"points in dataset.\")\n",
    "\n",
    "    return(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plotter(points):\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.scatter(points[:, 0].cpu().numpy(), points[:, 1].cpu().numpy(), alpha=0.6, edgecolors='k', s=20)\n",
    "    plt.title('Scatter Plot of q1 vs q2')\n",
    "    plt.xlabel('q1')\n",
    "    plt.ylabel('q2')\n",
    "    plt.xlim(-torch.pi, torch.pi)\n",
    "    plt.ylim(-torch.pi, torch.pi)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def make_dataset(points):\n",
    "\n",
    "    \"\"\"\n",
    "    Compute mass- and input matrix of all training points to reduce load in training.\n",
    "    Returns TensorDataset of (q, M_q, A_q). \n",
    "    \"\"\"\n",
    "\n",
    "    data_pairs = []\n",
    "    for point in points:\n",
    "        Mq_point, _, _ = dynamics.dynamical_matrices(rp, point, point)\n",
    "        Aq_point = dynamics.input_matrix(rp, point)\n",
    "        data_pairs.append((point, Mq_point, Aq_point))\n",
    "\n",
    "    points_tensor = torch.stack([pair[0] for pair in data_pairs])           # Tensor of all points\n",
    "    mass_matrices_tensor = torch.stack([pair[1] for pair in data_pairs])   # Tensor of all mass matrices\n",
    "    input_matrices_tensor = torch.stack([pair[2] for pair in data_pairs])  # Tensor of all input matrices\n",
    "\n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(points_tensor, mass_matrices_tensor, input_matrices_tensor)\n",
    "    return(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(dataset, batch_size = 512, train_part = 0.7):\n",
    "\n",
    "    train_size = int(train_part * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    # Create TensorDataset for both training and testing sets\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    # Create the DataLoader for both training and testing sets\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    return(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b5de6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c18c8fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import Double_Pendulum.Lumped_Mass.transforms as transforms\n",
    "\n",
    "\"\"\"\n",
    "This file contains the architectures of a number of Autoencoders used in my thesis. \n",
    "Currently it only contains a single, set architecture for learning both h1 and h2, \n",
    "however, this can be extended and made modular depending on __init__ conditions.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Autoencoder_double(nn.Module):\n",
    "    def __init__(self, rp):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.Softplus(),\n",
    "            nn.Linear(16, 2)\n",
    "        )\n",
    "        \n",
    "        self.rp = rp\n",
    "        \n",
    "    def encoder_theta_1_ana(self, q):\n",
    "        theta_1 = transforms.analytic_theta_1(self.rp, q).unsqueeze(0)\n",
    "        return theta_1, theta_1\n",
    "    \n",
    "    #This function is not used in the forward pass, but is useful for comparing learned to analytic theta_2\n",
    "    def encoder_theta_2_ana(self, q):\n",
    "        theta_2 = transforms.analytic_theta_2(self.rp, q).unsqueeze(0)\n",
    "        return theta_2, theta_2\n",
    "    \n",
    "    def theta_ana(self, q):\n",
    "        theta_1_ana, _ = torch.vmap(self.encoder_theta_1_ana)(q)\n",
    "        theta_2_ana, _ = torch.vmap(self.encoder_theta_2_ana)(q)\n",
    "        theta_ana = torch.cat((theta_1_ana, theta_2_ana), dim=1)\n",
    "        return theta_ana\n",
    "\n",
    "    def encoder_nn(self, q):\n",
    "        theta = self.encoder(q)\n",
    "        return theta, theta\n",
    "    \n",
    "    def decoder_nn(self, theta):\n",
    "        q_hat = self.decoder(theta)\n",
    "        return q_hat, q_hat\n",
    "    \n",
    "    def jacobian_enc(self, q):\n",
    "        print(q)\n",
    "        J_h, theta = torch.vmap(torch.func.jacfwd(self.encoder_nn, has_aux=True))(q)\n",
    "        return J_h, theta\n",
    "\n",
    "    def jacobian_dec(self, theta):\n",
    "        J_h_dec, q_hat = torch.vmap(torch.func.jacfwd(self.decoder_nn, has_aux=True))(theta)\n",
    "        return J_h_dec, q_hat\n",
    "    \n",
    "    def forward(self, q):\n",
    "        \n",
    "        J_h_1_ana, theta_1_ana = torch.vmap(torch.func.jacfwd(self.encoder_theta_1_ana, has_aux=True))(q)\n",
    "        J_h_2_ana, theta_2_ana = torch.vmap(torch.func.jacfwd(self.encoder_theta_2_ana, has_aux=True))(q)\n",
    "        J_h_ana = torch.cat((J_h_1_ana, J_h_2_ana), dim=1).float()\n",
    "        \n",
    "        \n",
    "        J_h, theta = torch.vmap(torch.func.jacfwd(self.encoder_nn, has_aux=True))(q)\n",
    "\n",
    "        J_h_dec, q_hat = self.jacobian_dec(theta)\n",
    "\n",
    "        return(theta, J_h, q_hat, J_h_dec, J_h_ana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_fun(q, q_hat, M_q, A_q, J_h, J_h_dec):\n",
    "    l_recon = F.mse_loss(q, q_hat, reduction=\"sum\")\n",
    "\n",
    "    \"\"\" If you pick J_h_inv to be the decoder Jacobian\"\"\"\n",
    "    J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "    J_h_inv = J_h_dec\n",
    "    J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "    M_th = J_h_inv_trans @ M_q @ J_h_inv  # Batch-wise diagonalization\n",
    "\n",
    "    \n",
    "    # Enforce inertial decoupling\n",
    "    #l_off_dia = torch.mean((M_th - torch.diagonal(M_th, dim1=1, dim2=2))**2)  # Penalize off-diagonal terms\n",
    "    l_off_dia = torch.mean((M_th[:, 0, 1])**2)\n",
    "    l_dia = torch.mean(-(torch.log(M_th[:, 0, 0])/(M_th[:, 0, 0]**0.8) + \n",
    "                         torch.log(M_th[:, 1, 1])/(M_th[:, 1, 1]**0.8)))\n",
    "\n",
    "    ## input decoupling loss\n",
    "    A_th = J_h_inv_trans @ A_q\n",
    "    l_input = torch.mean((A_th[:, 1]**2)) + torch.sum(((A_th[:, 0]-1)**2))\n",
    "\n",
    "\n",
    "    l_input = 10 * l_input\n",
    "\n",
    "    loss = l_recon + l_dia + l_off_dia + l_input \n",
    "    loss_terms = torch.tensor([l_recon, l_input, l_dia, l_off_dia])\n",
    "\n",
    "    return loss, loss_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model.load_state_dict(torch.load(load_path, weights_only=True))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def AE_model(rp, device, lr, num_epochs, q1_split, model_nr, train_dataloader, val_dataloader, current_time):\n",
    "   \n",
    "    model = Autoencoder_double(rp).to(device)  # Move model to GPU\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)#,  weight_decay=1e-6)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5 ** (1 / num_epochs))\n",
    "\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    start_time = time.time()\n",
    "    save_directory = os.path.join(os.getcwd(), \"Models/Split_AEs\")\n",
    "    os.makedirs(save_directory, exist_ok=True)\n",
    "    file_name = f\"Lumped_Mass_{current_time}_{model_nr}.pth\"\n",
    "    file_path = os.path.join(save_directory, file_name)\n",
    "\n",
    "    JSON = {\"Model nr\" : model_nr,\n",
    "            \"q1_low\" : q1_split[0],\n",
    "            \"q1_high\" : q1_split[1],\n",
    "            \"lr\" : lr,\n",
    "            \"epochs\" : num_epochs,\n",
    "            \"file_name\" : file_name}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_loss_terms = torch.zeros(4)\n",
    "        for index, (q, M_q, A_q) in enumerate(train_dataloader):\n",
    "            #batch_size = batch[0].shape[0]\n",
    "            q = q.to(device)\n",
    "            #q.requires_grad = True\n",
    "            #q_d = batch[0].to(device)\n",
    "\n",
    "            M_q = M_q.to(device)\n",
    "            A_q = A_q.to(device)\n",
    "            \n",
    "            theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)  \n",
    "            theta_ana = model.theta_ana(q)\n",
    "                    \n",
    "            loss, loss_terms = loss_fun(q, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_loss_terms += loss_terms\n",
    "        train_loss /= len(train_dataloader.dataset)\n",
    "        train_loss_terms /= len(train_dataloader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_loss_terms = torch.zeros(4)\n",
    "        with torch.no_grad():\n",
    "            for index, (q, M_q, A_q) in enumerate(val_dataloader):\n",
    "                q = q.to(device)\n",
    "                M_q = M_q.to(device)\n",
    "                A_q = A_q.to(device)\n",
    "\n",
    "                theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)\n",
    "                theta_ana = model.theta_ana(q)\n",
    "\n",
    "                loss, loss_terms = loss_fun(q, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "\n",
    "                J_h_inv = torch.linalg.inv(J_h)\n",
    "                J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "                M_th = J_h_inv_trans @ M_q @ J_h_inv \n",
    "\n",
    "                val_loss += loss.item()\n",
    "                val_loss_terms += loss_terms\n",
    "        val_loss /= len(val_dataloader.dataset)\n",
    "        val_loss_terms /= len(val_dataloader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "        epoch_duration = time.time() - start_time\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print(\n",
    "                f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Duration: {epoch_duration:.2f} seconds'\n",
    "                )\n",
    "\n",
    "            print(\n",
    "                f\"recon: {val_loss_terms[0].item():.4f}, \"\n",
    "                f\"input: {val_loss_terms[1].item():.4f}, \"\n",
    "                f\"dia: {val_loss_terms[2].item():.4f}, \"\n",
    "                f\"off_dia: {val_loss_terms[3].item():.4f}, \"\n",
    "            )\n",
    "        \n",
    "    return(model, train_losses, val_losses, file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print(f\"Model parameters saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses):\n",
    "\n",
    "    \"\"\"\n",
    "    Plots training and validation loss. \n",
    "    \"ylim\" and \"yscale\" should be enabled depending on the loss function.\n",
    "    \"\"\"\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    plt.plot(train_losses, label=\"Training Loss\")\n",
    "    plt.plot(val_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    #plt.ylim((-1, 40))\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss over Epochs\")\n",
    "    plt.grid(True)\n",
    "    #plt.yscale(\"log\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d44a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plotting.plotters_simple as plotters_simple\n",
    "\n",
    "def make_plot_dataloader(dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Takes the training dataset and returns a dataloader of every 10th point\n",
    "    to reduce visual clutter. \n",
    "    \"\"\"\n",
    "\n",
    "    points_tensor, mass_matrices_tensor, input_matrices_tensor = dataset.tensors\n",
    "    \n",
    "    plot_sampled = points_tensor[::10]\n",
    "    mass_sampled = mass_matrices_tensor[::10]\n",
    "    input_sampled = input_matrices_tensor[::10]\n",
    "\n",
    "    plot_dataset = TensorDataset(plot_sampled, mass_sampled, input_sampled)\n",
    "    plot_dataloader = DataLoader(plot_dataset, batch_size=len(plot_dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "    return(plot_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c160a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(model, plot_dataloader):\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for (q, M_q, A_q) in plot_dataloader:\n",
    "            q = q.to(device)\n",
    "            M_q = M_q.to(device)\n",
    "            A_q = A_q.to(device)\n",
    "\n",
    "            theta, J_h, q_hat, J_h_dec, J_h_ana = model(q)\n",
    "            theta_ana = model.theta_ana(q)\n",
    "            J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "            J_h_inv = J_h_dec\n",
    "            J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "            #J_h_inv = J_h_trans\n",
    "            #J_h_inv_trans = J_h\n",
    "            M_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "            A_th = (J_h_inv_trans @ A_q).squeeze(-1)\n",
    "\n",
    "            off_dia = M_th[:, 0, 1]\n",
    "            diag_elements = M_th[:, [0, 1], [0, 1]]\n",
    "            #diag_product = torch.sqrt(diag_elements[:, 0] * diag_elements[:, 1])# + 1e-8\n",
    "            diag_product = diag_elements[:, 1]\n",
    "            M_th_ratio = off_dia/diag_product\n",
    "\n",
    "            J_ortho = J_h @ J_h_trans        \n",
    "            J_h_identity = J_h @ J_h_inv  # Batch-wise multiplication\n",
    "            l_J_inv = ((torch.eye(J_h_identity.size(-1), device=J_h_identity.device) - J_h_identity)**2).sum(dim=(1,2)) \n",
    "            l_J_ortho = ((torch.eye(J_h_identity.size(-1), device=J_h_identity.device) - J_ortho)**2).sum(dim=(1,2))\n",
    "\n",
    "            \n",
    "            #plotters_simple.plot_3d_double(q, theta_ana[:, 0], theta[:, 0], \"h0\", \"analytical\", \"learned\", \"h0\", device)\n",
    "            #plotters_simple.plot_3d_double(q, theta_ana[:, 1], theta[:, 1], \"h1\", \"analytical\", \"learned\", \"h1\", device)\n",
    "\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 0, 0], J_h[:, 0, 0], \"J_h_00\", \"analytical\", \"learned\", \"J_h_00\", device)\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 0, 1], J_h[:, 0, 1], \"J_h_01\", \"analytical\", \"learned\", \"J_h_01\", device)\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 1, 0], J_h[:, 1, 0], \"J_h_10\", \"analytical\", \"learned\", \"J_h_10\", device)\n",
    "            #plotters_simple.plot_3d_double(q, J_h_ana[:, 1, 1], J_h[:, 1, 1], \"J_h_11\", \"analytical\", \"learned\", \"J_h_11\", device)\n",
    "\n",
    "            plotters_simple.plot_3d_double(q, M_th_ratio, M_th_ratio, \"Inertial decoupling\", \"capped\", \"full\", \"M_th_ratio\", device, z_limits=(-1,1))\n",
    "\n",
    "            plotters_simple.plot_3d_double(q, A_th[:, 0], A_th[:, 1], \"Input decoupling\", \"A0\", \"A1\", \"A\", device)\n",
    "\n",
    "            #plotters_simple.plot_3d_double(q, l_J_ortho, l_J_inv, \"Jacobian properties\", \"Orthogonality J_h\", \"J_h@J_h_inv - I\", \"J_h\", device)\n",
    "            \n",
    "            plotters_simple.plot_3d_double(q, M_th[:, 0, 0], M_th[:, 0, 1], \"M_th\", \"00\", \"01\", \"M_th\", device)\n",
    "            plotters_simple.plot_3d_double(q, M_th[:, 1, 0], M_th[:, 1, 1], \"M_th\", \"10\", \"11\", \"M_th\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1aef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_1_single(model, q):\n",
    "    theta = model.encoder_nn(q)[0]\n",
    "    return theta[:, 0].detach()\n",
    "    \n",
    "def theta_2_single(model, q):\n",
    "    theta = model.encoder_nn(q)[0]\n",
    "    return theta[:, 1].detach()\n",
    "    \n",
    "def q_hat_1_single(model, theta):\n",
    "    q_hat = model.decoder_nn(theta)[0]\n",
    "    return q_hat[:, 0].detach()\n",
    "    \n",
    "def q_hat_2_single(model, theta):\n",
    "    q_hat = model.decoder_nn(theta)[0]\n",
    "    return q_hat[:, 1].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81624d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_splits = [(-torch.pi, -torch.pi/2),\n",
    "             (-torch.pi/2, 0),\n",
    "             (0, torch.pi/2)]\n",
    "\n",
    "q1_splits = [(-torch.pi, torch.pi)]\n",
    "\n",
    "angle_margin = torch.pi/10\n",
    "\n",
    "batch_size = 512\n",
    "train_part = 0.7\n",
    "\n",
    "rp = robot_parameters.LUMPED_PARAMETERS\n",
    "num_epochs = 1001\n",
    "lr = 1e-3\n",
    "\n",
    "\n",
    "plt.ion()\n",
    "mapping_functions_list = []\n",
    "model_list = []\n",
    "train_losses_list = []\n",
    "val_losses_list = []\n",
    "file_path_list = []\n",
    "\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "for model_nr, q1_split in enumerate(q1_splits):\n",
    "    points = masked_points(q1_split, angle_margin)\n",
    "    #points_plotter(points)\n",
    "    dataset = make_dataset(points)\n",
    "    (train_dataloader, val_dataloader) = make_dataloaders(dataset=dataset, batch_size=batch_size, train_part=train_part)\n",
    "\n",
    "    outputs = []\n",
    "    model, train_losses, val_losses, file_path = AE_model(rp, device, lr, num_epochs, q1_split, model_nr, train_dataloader, \n",
    "                                                          val_dataloader, current_time)\n",
    "    \n",
    "    plot_loss(train_losses, val_losses)\n",
    "    mapping_functions = (partial(theta_1_single,model), \n",
    "                         partial(theta_2_single,model), \n",
    "                         partial(q_hat_1_single,model), \n",
    "                         partial(q_hat_2_single,model))\n",
    "    \n",
    "    mapping_functions_list.append(mapping_functions)\n",
    "    model_list.append(model)\n",
    "    train_losses_list.append(train_losses)\n",
    "    val_losses_list.append(val_losses)\n",
    "    file_path_list.append(file_path)\n",
    "    \n",
    "    save_model(model, file_path)\n",
    "\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, device=device, \n",
    "                                            mapping_functions_list=mapping_functions_list, mask_splits=q1_splits)\n",
    "th_plotter.make_figure(\"theta_learned_full_test.png\")\n",
    "th_plotter.make_animation(\"theta_learned_full_test.mp4\", duration = 4, fps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "%matplotlib widget\n",
    "\n",
    "for q1_split in q1_splits:\n",
    "    points = masked_points(q1_split, angle_margin)\n",
    "    points_plotter(points[::5])\n",
    "    dataset = make_dataset(points)\n",
    "    plot_model_performance(model_list[0], make_plot_dataloader(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efe8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_functions = (partial(torch.vmap(transforms.analytic_theta_1, in_dims=(None, 0)), rp), \n",
    "                     partial(torch.vmap(transforms.analytic_theta_2, in_dims=(None, 0)), rp))\n",
    "mapping_functions_list = [mapping_functions]\n",
    "mask_splits = [(-torch.pi, torch.pi)]\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, device=device, n_lines=50, mapping_functions_list=mapping_functions_list, \n",
    "                                            mask_splits=mask_splits)\n",
    "#th_plotter.make_figure(\"theta_subset_5_2_image.png\")\n",
    "#th_plotter.make_animation(\"theta_subset_5_2.mp4\", duration = 4, fps = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6034ce",
   "metadata": {},
   "source": [
    "## OLD LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b667ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### OLD LOSS FUNCTION\n",
    "\n",
    "def loss_fun(q, q_hat, m_matrix, input_matrix, J_h, J_h_dec):\n",
    "    l_recon = F.mse_loss(q, q_hat, reduction=\"sum\")\n",
    "\n",
    "    \"\"\" If you pick J_h_inv to be the inverse/transpose of Jh\"\"\"\n",
    "    #J_h_inv = torch.transpose(J_h, 1, 2)\n",
    "    #J_h_trans = J_h_inv\n",
    "    #J_h_inv_trans = J_h\n",
    "\n",
    "    \"\"\" If you pick J_h_inv to be the decoder Jacobian\"\"\"\n",
    "    J_h_trans = torch.transpose(J_h, 1, 2)\n",
    "    J_h_inv = J_h_dec\n",
    "    J_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "    \n",
    "\n",
    "    # Orthogonality ensures that the transformation does not \"stretch\" too much\n",
    "    # NOTE:, if using J_h_inv = J_h^T, J_ortho and J_h_identity are the _same_ thing\n",
    "    J_ortho = J_h @ J_h_trans\n",
    "    J_ortho_inv = J_h_inv @ J_h_inv_trans\n",
    "    \n",
    "    J_h_identity = J_h @ J_h_inv  # Batch-wise multiplication\n",
    "    identity_target = torch.eye(J_h_identity.size(-1), device=J_h_identity.device)\n",
    "\n",
    "\n",
    "    # Enforce that decoder Jacobian is the inverse of the encoder Jacobian\n",
    "    ## USE ONLY WHEN J_h_inv == J_h_dec\n",
    "    l_J_inv = torch.sum((identity_target - J_h_identity)**2)  # Frobenius norm\n",
    "    l_J_ortho_inv = torch.sum((identity_target - J_ortho_inv)**2)  # Frobenius norm\\n\",\n",
    "\n",
    "    # Enforce that the Jacobian is orthogonal\n",
    "    l_J_ortho = torch.sum((identity_target - J_ortho)**2)  # Frobenius norm\n",
    "\n",
    "    diag_matrix = J_h_inv_trans @ m_matrix @ J_h_inv  # Batch-wise diagonalization\n",
    "    \"\"\"\n",
    "    # Enforce inertial decoupling (diagonal loss)\n",
    "    off_dia = diag_matrix[:, 0, 1]\n",
    "    trace = torch.einsum('bii->b', diag_matrix) \n",
    "    M_th_ratio = off_dia/trace\n",
    "    l_diag = nn.MSELoss()(M_th_ratio, torch.zeros((M_th_ratio.size())).to(device))\n",
    "    \"\"\"\n",
    "    \n",
    "    # Enforce inertial decoupling\n",
    "    ## Slight concern is that this sends all values of M_th to 0. Hopefully l_J_ortho avoids this\n",
    "    dia_mask = torch.eye(2).unsqueeze(0).repeat(diag_matrix.size(0), 1, 1)\n",
    "    l_off_dia = torch.sum(diag_matrix**2) - torch.sum(torch.diagonal(diag_matrix, dim1=1, dim2=2)**2)  # Penalize off-diagonal terms\n",
    "    #l_off_dia = torch.sum((diag_matrix[:, 0, 1])**2)\n",
    "    l_dia = torch.sum(-(torch.log(diag_matrix[:, 0, 0])/(diag_matrix[:, 0, 0]**0.8) + \n",
    "                        torch.log(diag_matrix[:, 1, 1])/(diag_matrix[:, 0, 0]**0.8)))\n",
    "    #l_dia = 0\n",
    "\n",
    "    ## input decoupling loss\n",
    "    input_x = J_h_inv_trans @ input_matrix\n",
    "    l_input = torch.sum((input_x[:, 1]**2)) + torch.sum(((input_x[:, 0]-1)**2))\n",
    "\n",
    "\n",
    "\n",
    "    l_J_inv = 0\n",
    "    l_J_ortho = 0 # 0.1 * l_J_ortho\n",
    "    l_J_ortho_inv = 0 # 0.1 * l_J_ortho_inv\n",
    "    l_input = 10 * l_input\n",
    "\n",
    "    loss = l_recon + l_dia + l_off_dia + l_input #+ l_J_inv #+ l_J_ortho + l_J_ortho_inv\n",
    "    loss_terms = torch.tensor([l_recon, l_input, l_dia, l_off_dia, l_J_ortho, l_J_inv, l_J_ortho_inv])\n",
    "\n",
    "    return loss, loss_terms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
