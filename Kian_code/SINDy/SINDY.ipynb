{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([0, 0, 0, 1, 1, 2]), array([0, 1, 2, 1, 2, 2]))\n",
      "['1', 'z0', 'z1', 'z2', 'sin(z0)', 'sin(z1)', 'sin(z2)', 'cos(z0)', 'cos(z1)', 'cos(z2)']\n",
      "tensor([[ 1.0000,  1.0000,  2.0000,  3.0000, -0.8415, -0.9093, -0.1411,  0.5403,\n",
      "         -0.4161, -0.9900],\n",
      "        [ 1.0000,  4.0000,  0.0000,  6.0000,  0.7568, -0.0000,  0.2794, -0.6536,\n",
      "          1.0000,  0.9602]], device='cuda:0')\n",
      "(array([0, 0, 0, 1, 1, 2]), array([0, 1, 2, 1, 2, 2]))\n",
      "Learned coefficients: [[ 0.          0.         -0.          0.          0.          0.\n",
      "   0.         -0.          0.          0.          0.         -0.\n",
      "   0.         -0.         -0.          0.01056242]\n",
      " [ 0.         -0.          0.         -0.         -0.         -0.\n",
      "  -0.          0.         -0.         -0.         -0.          0.\n",
      "  -0.          0.          0.         -0.01056242]\n",
      " [ 0.          0.         -0.          0.          0.          0.\n",
      "   0.         -0.          0.          0.          0.         -0.\n",
      "   0.         -0.         -0.          0.01056242]]\n",
      "Approximated output: [[ 0.09506175 -0.09506173  0.09506175]\n",
      " [ 0.380247   -0.38024694  0.380247  ]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "class SINDyLibrary():\n",
    "    def __init__(self,\n",
    "                 latent_dim=3,\n",
    "                 include_biases=True,\n",
    "                 include_states=True,\n",
    "                 include_sin=True,\n",
    "                 include_cos=True,\n",
    "                 poly_order=1,\n",
    "                 include_sqrt=False,\n",
    "                 include_inverse=False,\n",
    "                 include_exp=False,\n",
    "                 include_sign_sqrt_of_diff=False,\n",
    "                 device='cuda:0'):\n",
    "\n",
    "        self.device = device\n",
    "        self.candidate_functions = []\n",
    "        self.feature_names = []\n",
    "        # initialize lib with biasses\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.include_biases = include_biases\n",
    "        self.include_states = include_states\n",
    "        self.include_sin = include_sin\n",
    "        self.include_cos = include_cos\n",
    "        self.include_exp = include_exp\n",
    "        self.include_inverse = include_inverse\n",
    "        self.poly_order = poly_order\n",
    "        self.include_sqrt = include_sqrt\n",
    "        self.include_singn_sqrt_of_diff = include_sign_sqrt_of_diff\n",
    "\n",
    "        # fit for functions and feature names\n",
    "        self.fit()\n",
    "        self.number_candidate_functions = len(self.feature_names)\n",
    "\n",
    "    def biases(self, z):\n",
    "        return torch.ones(z.shape[0], 1, device=self.device)\n",
    "\n",
    "    @staticmethod\n",
    "    def states(z):\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def sin(z):\n",
    "        return - torch.sin(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def cos(z):\n",
    "        return torch.cos(z)\n",
    "\n",
    "    def multiply_pairs(self, z):\n",
    "        result = []\n",
    "        # for idx1, idx2 in self.idx_combis_commutative:\n",
    "        #     res = z[:, idx1] * z[:, idx2]\n",
    "        #     res = res.reshape(-1, 1)\n",
    "        #     result.append(res)\n",
    "        result = z[:, self.idx_new[0]] * z[:, self.idx_new[1]]\n",
    "        return result\n",
    "        # return torch.cat(result, axis=1)\n",
    "\n",
    "    @staticmethod\n",
    "    def inverse(z):\n",
    "        return 1 / (z + 1e-4)\n",
    "\n",
    "    def poly_deg_2(self, z):\n",
    "        result = z[:, self.idx_new[0]] * z[:, self.idx_new[1]]\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def sqrt(z):\n",
    "        return torch.sqrt(z)\n",
    "\n",
    "    @staticmethod\n",
    "    def exp(z):\n",
    "        return torch.exp(z)\n",
    "\n",
    "    def sing_sqrt_diff_pairs(self, z):\n",
    "        result = []\n",
    "        for idx1, idx2 in self.idx_combis_commutative:\n",
    "            res = torch.sign(z[:, idx1] - z[:, idx2]) * torch.sqrt(torch.abs(z[:, idx1] - z[:, idx2]))\n",
    "            res = res.reshape(-1, 1)\n",
    "            result.append(res)\n",
    "        return torch.cat(result, axis=1)\n",
    "\n",
    "    def fit(self):\n",
    "        ## generate all possible pairs of z variable indices\n",
    "        possible_indicies = list(range(self.latent_dim))\n",
    "        permuts = itertools.product(possible_indicies, possible_indicies)\n",
    "        #print(\"permuts:\\n\", list(permuts))\n",
    "        permuts = [p for p in permuts if not p[0] == p[1]]\n",
    "        self.idx_combis_non_commutative = permuts\n",
    "        self.idx_combis_commutative = list(set([tuple(sorted(list(p))) for p in permuts]))\n",
    "\n",
    "        self.idx_new = np.triu_indices(self.latent_dim)\n",
    "        print(self.idx_new)\n",
    "\n",
    "        if self.include_biases:\n",
    "            self.candidate_functions.append(self.biases)\n",
    "            names = ['1']\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_states:\n",
    "            self.candidate_functions.append(self.states)\n",
    "            names = [f'z{i}' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_sin:\n",
    "            self.candidate_functions.append(self.sin)\n",
    "            names = [f'sin(z{i})' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_cos:\n",
    "            self.candidate_functions.append(self.cos)\n",
    "            names = [f'cos(z{i})' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_inverse:\n",
    "            self.candidate_functions.append(self.inverse)\n",
    "            names = [f'1/z{i}' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.poly_order == 2:\n",
    "            self.candidate_functions.append(self.poly_deg_2)\n",
    "            names = [f'z{idx1}*z{idx2}' for idx1, idx2 in zip(self.idx_new[0], self.idx_new[1])]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_sqrt:\n",
    "            self.candidate_functions.append(self.sqrt)\n",
    "            names = [f'sqrt(z{i})' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_exp:\n",
    "            self.candidate_functions.append(self.exp)\n",
    "            names = [f'exp(z{i})' for i in range(self.latent_dim)]\n",
    "            self.feature_names.extend(names)\n",
    "        if self.include_singn_sqrt_of_diff:\n",
    "            self.candidate_functions.append(self.sing_sqrt_diff_pairs)\n",
    "            names = [f'sign(z{idx1}-z{idx2})*sqrt(|z{idx1}-z{idx2}|)'\n",
    "                     for idx1, idx2 in self.idx_combis_commutative]\n",
    "            self.feature_names.extend(names)\n",
    "\n",
    "    def get_feature_names(self, ):\n",
    "        return self.feature_names\n",
    "\n",
    "    def transform(self, z):\n",
    "        theta = [cand_func(z) for cand_func in self.candidate_functions]\n",
    "        out = torch.cat(theta, axis=1)\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # some test for the SINDy lib\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    z = torch.tensor([[1, 2, 3], [4, 0, 6]]).to(device)\n",
    "    sl = SINDyLibrary()\n",
    "    theta = sl.transform(z)\n",
    "    print(sl.feature_names)\n",
    "    print(theta)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Step 1: Generate the candidate features (basis functions)\n",
    "z = torch.tensor([[1, 2, 3], [4, 0, 6]]).to(device)\n",
    "sl = SINDyLibrary(latent_dim=3, include_sin=True, include_cos=True, poly_order=2)\n",
    "theta = sl.transform(z)\n",
    "\n",
    "# Step 2: Use a sparse regression method (e.g., LASSO) to learn the coefficients\n",
    "# Assuming `y` is the derivative or target you're trying to model (e.g., dx/dt)\n",
    "y = torch.tensor([[0.1, -0.2, 0.3], [0.4, -0.5, 0.6]]).to(device)\n",
    "\n",
    "# Use LASSO regression to identify important basis functions\n",
    "lasso = Lasso(alpha=0.1)  # Regularization parameter\n",
    "lasso.fit(theta.cpu().numpy(), y.cpu().numpy())\n",
    "\n",
    "# Step 3: Extract and interpret the coefficients\n",
    "coefficients = lasso.coef_\n",
    "print(\"Learned coefficients:\", coefficients)\n",
    "\n",
    "# Step 4: Use the learned coefficients to approximate the function\n",
    "approximated_function = np.dot(theta.cpu().numpy(), coefficients.T)\n",
    "print(\"Approximated output:\", approximated_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
