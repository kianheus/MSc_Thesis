{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b55a57bd",
   "metadata": {},
   "source": [
    "### Autoencoder training to learn decoupling coordinates of Series Elastic Actuator\n",
    "\n",
    "Consider a series elastic actuator with motor inertia $I_0$, link inertia $I_1$ and spring stiffness $k$. <br>The (actuated) motor angle $\\theta_0$ and (unactuated) link angle $\\theta_1$ are linked through the spring. <br>This standard formulation results in an inertially and input decoupled system in Euler-Lagrange formulation:\n",
    "\n",
    "$\\mathbf{M} \\ddot{\\theta} + \\mathbf{G}(\\theta) = \\mathbf{A} u$\n",
    "\n",
    "With mass matrix:\n",
    "\n",
    "$\\mathbf{M}(\\theta) = \\begin{bmatrix} I_0 & 0 \\\\\\ 0 & I_1 \\end{bmatrix}$\n",
    "\n",
    "potential matrix:\n",
    "\n",
    "$\\mathbf{G}(\\theta) = \\begin{bmatrix} \\phantom{-.} k (\\theta_0 - \\theta_1)\\\\\\ -k (\\theta_0 - \\theta_1)\\end{bmatrix}$ \n",
    "\n",
    "and input matrix:\n",
    "\n",
    "$\\mathbf{A}(\\theta) = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03064828",
   "metadata": {},
   "source": [
    "### Relative angles\n",
    "When instead one chooses as second coordinate the relative angle, the system is no longer inertially decoupled.<br>Choose:<br>$q_0 = \\theta_0$ (motor angle)<br>$q_1 = \\theta_1 - \\theta_0$ (relative angle)\n",
    "\n",
    "This result in the following Euler-Lagrange formulation:\n",
    "\n",
    "Kinetic energy:\n",
    "\n",
    "$T = \\frac{1}{2} I_0 \\dot{q}_0^2 + \\frac{1}{2} I_1 (\\dot{q}_1+\\dot{q}_0)^2$\n",
    "\n",
    "Potential energy:\n",
    "\n",
    "$V = \\frac{1}{2} k q_1^2$\n",
    "\n",
    "Lagrangian L = T - V:\n",
    "\n",
    "$L = \\frac{1}{2} I_0 \\dot{q}_0^2 + \\frac{1}{2} I_1 (\\dot{q}_1+\\dot{q}_0)^2 - \\frac{1}{2} k q_1^2$\n",
    "\n",
    "We know that we must have:<br>\n",
    "$\\frac{d}{dt}(\\frac{\\partial L}{\\partial \\dot{\\theta_i}}) - \\frac{\\partial L}{\\partial \\theta_i} = 0$\n",
    "\n",
    "With partial derivatives:<br>\n",
    "$\\frac{d}{dt}(\\frac{\\partial L}{\\partial \\dot{\\theta_0}}) = (I_0 + I_1) \\ddot{q}_0 + I_1 \\ddot{q}_1$, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\frac{\\partial L}{\\partial \\theta_0} = 0$\n",
    "\n",
    "$\\frac{d}{dt}(\\frac{\\partial L}{\\partial \\dot{\\theta_1}}) = I_1 \\ddot{q}_0 + I_1 \\ddot{q}_1$, &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $\\frac{\\partial L}{\\partial \\theta_1} = k \\theta_1$\n",
    "\n",
    "Leading to the following matrices:\n",
    "\n",
    "$\\mathbf{M}(q) = \\begin{bmatrix} I_0 + I_1 & I_1 \\\\\\ I_1 & I_1 \\end{bmatrix}$\n",
    "\n",
    "$\\mathbf{G}(q) = \\begin{bmatrix} 0\\\\\\ k q_1\\end{bmatrix}$ \n",
    "\n",
    "$\\mathbf{A}(q) = \\begin{bmatrix} 1 \\\\\\ 0 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d480f1b1",
   "metadata": {},
   "source": [
    "### So what should the autoencoder learn?\n",
    "\n",
    "Starting with the \"naive\" coordinates $q$, the only choice of coordinates that leads to inertial and input decoupling is $\\theta$, or some trivial re-scaling of $\\theta$. \n",
    "\n",
    "The autoencoder is thus trying to approximate:\n",
    "\n",
    "$\\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix} = \\begin{bmatrix} q_0 \\\\ q_0 + q_1 \\end{bmatrix}$\n",
    "\n",
    "With Jacobian:\n",
    "\n",
    "$J_h(q) = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de701e",
   "metadata": {},
   "source": [
    "### Let's go (starting with imports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"../..\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "import Series_Elastic_Actuator.robot_parameters_SEA as robot_parameters_SEA\n",
    "import Series_Elastic_Actuator.transforms_SEA as transforms_SEA\n",
    "import Series_Elastic_Actuator.dynamics_SEA as dynamics_SEA\n",
    "\n",
    "import Series_Elastic_Actuator.Learning.training_data_SEA as training_data_SEA\n",
    "import Plotting.theta_visualiser as theta_visualizer\n",
    "\n",
    "import Series_Elastic_Actuator.Learning.autoencoders_SEA as autoencoders_SEA\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa26f4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "rp = robot_parameters_SEA.SEA_PARAMETERS\n",
    "print(rp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49075fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create points (q0, q1) between -pi and pi for training\n",
    "points = training_data_SEA.points.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ef88e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def points_plotter(points, extend = None, save = False, file_path = None):\n",
    "\n",
    "\t\"\"\" \n",
    "\tSimple plotter function which visualizes the points used for training of the Autoencoder. \n",
    "\t\"\"\"\n",
    "\t \n",
    "\tplt.figure(figsize=(6, 6))\n",
    "\tplt.scatter(points[:, 0].cpu().numpy(), points[:, 1].cpu().numpy(), alpha=0.6, edgecolors='k', s=20)\n",
    "\tplt.title('Scatter Plot of q0 vs q1')\n",
    "\tplt.xlabel('q0')\n",
    "\tplt.ylabel('q1')\n",
    "\tplt.xlim(-2*torch.pi, 2*torch.pi)\n",
    "\tplt.ylim(-2*torch.pi, 2*torch.pi)\n",
    "\tplt.grid(True)\n",
    "\t\n",
    "\tif save:\n",
    "\t\tplt.savefig(file_path, dpi=800)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c8c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_plotter(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e3ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(points):\n",
    "\n",
    "\t\"\"\"\n",
    "\tCompute mass- and input matrix of all training points to reduce load in training.\n",
    "\tReturns TensorDataset of (q, M_q, A_q). \n",
    "\t\"\"\"\n",
    "\t\n",
    "\tdata_pairs = []\n",
    "\tfor point in points:\n",
    "\t\tMq_point, _ = dynamics_SEA.dynamical_matrices(rp, point, point)\n",
    "\t\tAq_point = dynamics_SEA.input_matrix()\n",
    "\t\tdata_pairs.append((point, Mq_point, Aq_point))\n",
    "\n",
    "\tpoints_tensor = torch.stack([pair[0] for pair in data_pairs])           # Tensor of all points\n",
    "\tmass_matrices_tensor = torch.stack([pair[1] for pair in data_pairs])   # Tensor of all mass matrices\n",
    "\tinput_matrices_tensor = torch.stack([pair[2] for pair in data_pairs])  # Tensor of all input matrices\n",
    "\n",
    "\t# Create TensorDataset\n",
    "\tdataset = TensorDataset(points_tensor, mass_matrices_tensor, input_matrices_tensor)\n",
    "\treturn(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2bb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataloaders(dataset, batch_size = 512, train_part = 0.7):\n",
    "\n",
    "\t\"\"\"\n",
    "\tCreates a training and validation dataloader from an input dataset, based on \n",
    "\tbatch size and the ratio train_part. \n",
    "\t\"\"\"\n",
    "\n",
    "\ttrain_size = int(train_part * len(dataset))\n",
    "\tval_size = len(dataset) - train_size\n",
    "\n",
    "\t# Create TensorDataset for both training and testing sets\n",
    "\ttrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\t# Create the DataLoader for both training and testing sets\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\tval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "\treturn(train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900a6207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Need to pick weights through some optimization scheme\n",
    "w_recon\t\t\t= 4.\n",
    "w_diag\t\t\t= 1.25\n",
    "w_off_diag\t\t= 1.25\n",
    "w_input \t\t= 1.\n",
    "w_input_jac\t\t= 0.8\n",
    "w_theta\t\t\t= 1.\n",
    "w_J_eye \t\t= 1.\n",
    "\n",
    "\n",
    "def loss_fun(q, theta, q_hat, M_q, A_q, J_h_enc, J_h_dec):\n",
    "\n",
    "\t\"\"\"\n",
    "\tLoss function for training the Autoencoder. Loss terms are the following:\n",
    "\tl_recon:    \tLoss between input- and reconstructed variable. (MSE)\n",
    "\tl_diag:      \tLoss on diagonal terms of mass matrix in theta-space. \n",
    "\t\t\t\t\t(mean of normalized negative log-loss)\n",
    "\tl_off_dia:  \tLoss of off-diagonal terms of mass matrix in theta-space. (MSE)\n",
    "\tl_input:    \tLoss to drive input matrix in theta-space to [1, 0]^T (MSE)\n",
    "\tl_input_jac:\tLoss based on 1st Jacobian row similarity to A_q column\n",
    "\tl_theta: \t\tLoss on the first coordinate theta_0, based on analytic knowledge\n",
    "\tl_J_eye: \t\tLoss on J_enc@J_dec similarity to identity\n",
    "\t\"\"\"\n",
    "\n",
    "\t\n",
    "\n",
    "\t# Calculate forward and inverse Jacobians\n",
    "\tJ_h = J_h_enc\n",
    "\tJ_h_trans = torch.transpose(J_h, 1, 2)\n",
    "\tJ_h_inv = J_h_dec\n",
    "\tJ_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "\tM_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "\tA_th = J_h_inv_trans @ A_q\n",
    "\n",
    "\t# 1. Standard reconstruction loss\n",
    "\tdelta = torch.atan2(torch.sin(q-q_hat), torch.cos(q-q_hat))\n",
    "\tl_recon = F.mse_loss(delta, torch.zeros_like(delta), reduction=\"mean\")\n",
    "\n",
    "\t# 2. Inertial decoupling, diagonal\n",
    "\tdiag_values = torch.diagonal(M_th, dim1=1, dim2=2)\n",
    "\tl_diag = torch.mean((-1 + torch.exp(-(diag_values - 1))) * (diag_values < 1).float())  # Shape: (batch_size, 2)\n",
    "\n",
    "\t# 3. Inertial decoupling, off-diagonal\n",
    "\tl_off_diag = torch.mean((M_th[:, 0, 1])**2)\n",
    "\n",
    "\t# 4. Input decoupling\n",
    "\tl_input = torch.mean((A_th[:, 1]**2))\n",
    "\n",
    "\t# 5. Jacobian loss, inspired by Pietro Pustina's paper on input decoupling: # https://arxiv.org/pdf/2306.07258\n",
    "\tl_input_jac = F.mse_loss(J_h[:, 0, :], A_q[:, :, 0], reduction=\"mean\")\n",
    "\n",
    "\t# 6. On the first coordinate theta, again from Pietro Pustina's analytic formulation\n",
    "\ttheta_ana = torch.vmap(transforms_SEA.analytic_theta, in_dims = (None, 0))(rp, q)\n",
    "\tl_theta = F.mse_loss(theta[:, 0], theta_ana[:, 0], reduction=\"mean\")\n",
    "\n",
    "\t# 7. Jacobian identity loss\n",
    "\tl_J_eye = F.mse_loss( torch.bmm(J_h, J_h_inv), \n",
    "\t\t\t\t\t\ttorch.eye(2,device=J_h.device).unsqueeze(0).repeat(J_h.size(0),1,1),\n",
    "\t\t\t\t\t\treduction='mean' )\n",
    "\n",
    "\n",
    "\t\n",
    "\t\"\"\"\n",
    "\t# ALTERNATIVE LOSS: For inertial decoupling, error w.r.t. identity matrix. \n",
    "\t#print(torch.eye(M_th.size(-1)))]).\n",
    "\tM_eye = torch.eye(M_th.size(-1), device=M_th.device, dtype=M_th.dtype).expand(M_th.size())\n",
    "\tl_inertia_eye = F.mse_loss(M_th, M_eye, reduction=\"mean\")\n",
    "\t\"\"\"\n",
    "\n",
    "\tloss_terms = torch.tensor([l_recon * w_recon, l_diag * w_diag, l_off_diag * w_off_diag, \n",
    "\t                           l_input * w_input, l_input_jac * w_input_jac, l_theta * w_theta,  l_J_eye * w_J_eye])\n",
    "\tloss_sum = (l_recon * w_recon + l_diag * w_diag + l_off_diag * w_off_diag + l_input * w_input + \n",
    "\t\t\t\tl_input_jac * w_input_jac + l_theta * w_theta + l_J_eye * w_J_eye)\n",
    "\n",
    "\treturn loss_sum, loss_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec57702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_AE_model(rp, device, lr, num_epochs, q0_split, train_dataloader, val_dataloader, current_time, dir_name = None):\n",
    "\n",
    "\t\"\"\"\n",
    "\tExecutes training loop for Autoencoder\n",
    "\t\"\"\"\n",
    "\n",
    "\t# Very big fluff to make folders and save paths\n",
    "\tif dir_name is None:\n",
    "\t\tmodel = autoencoders_SEA.Autoencoder_double(rp).to(device)  # Move model to GPU\n",
    "\t\tdir_name = f\"NN_{current_time}\"\n",
    "\t\tfile_counter = 0\n",
    "\telse:\n",
    "\t\tdir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "\t\tnn_filename = dir_name + \"_0.pth\"\n",
    "\t\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\t\tfile_counter = 0\n",
    "\t\twhile os.path.isfile(nn_filepath):\n",
    "\t\t\tprint(\"Looking for latest weights and bias file.\")\n",
    "\t\t\tfile_counter += 1\n",
    "\t\t\tnn_filename = nn_filename[:-6] + \"_\" + str(file_counter) + \".pth\"\n",
    "\t\t\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "\t\tmodel_path = os.path.join(\"Models\", dir_name, dir_name + \"_\" + str(file_counter-1) + \".pth\")\n",
    "\t\tmodel = autoencoders_SEA.Autoencoder_double(rp).to(device)  # Initialize model architecture\n",
    "\t\tmodel.load_state_dict(torch.load(model_path, weights_only=True, map_location=device))  # Load weights\n",
    "\n",
    "\toptimizer = torch.optim.Adam(model.parameters(), lr=lr)#,  weight_decay=1e-6)\n",
    "\tscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5 ** (1 / num_epochs))\n",
    "\n",
    "\tstart_time = time.time()       \n",
    "\tdir_path = os.path.join(os.getcwd(), \"Models\", dir_name)\n",
    "\tos.makedirs(dir_path, exist_ok=True)\n",
    "\tnn_filename = dir_name + \"_\" + str(file_counter) + \".pth\"\n",
    "\tnn_filepath = os.path.join(dir_path, nn_filename)\n",
    "\n",
    "\tmetadata = {\"q0_low\" : q0_split[0],\n",
    "\t\t\t\t\"q0_high\" : q0_split[1],\n",
    "\t\t\t\t\"lr\" : lr,\n",
    "\t\t\t\t\"epochs\" : num_epochs,\n",
    "\t\t\t\t\"file_name\" : nn_filename,\n",
    "\t\t\t\t\"loss_weights\": {\n",
    "\t\t\t\t\t\"w_recon\": w_recon,\n",
    "\t\t\t\t\t\"w_diag\": w_diag,\n",
    "\t\t\t\t\t\"w_off_diag\": w_off_diag,\n",
    "\t\t\t\t\t\"w_input\": w_input,\n",
    "\t\t\t\t\t\"w_input_jac\": w_input_jac,\n",
    "\t\t\t\t\t\"w_theta\": w_theta,\n",
    "\t\t\t\t\t\"w_J_eye\": w_J_eye\n",
    "\t\t\t\t}}\n",
    "\tmetadata[\"architecture\"] = {\n",
    "    \"encoder\": autoencoders_SEA.summarize_sequential(model.enc),\n",
    "    \"decoder\": autoencoders_SEA.summarize_sequential(model.dec)\n",
    "\t}\n",
    "\t\n",
    "\tmetadata_filename = \"metadata_\" + str(file_counter) + \".json\"\n",
    "\tmetadata_filepath = os.path.join(dir_path, metadata_filename)\n",
    "\twith open(metadata_filepath, \"w\") as f:\n",
    "\t\tjson.dump(metadata, f, indent=4)\n",
    "\n",
    "\t# Initialise training parameters\n",
    "\ttrain_losses = []\n",
    "\tval_losses = []\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\n",
    "\t\t# Training phase\n",
    "\t\tmodel.train()\n",
    "\t\ttrain_loss = 0\n",
    "\t\ttrain_loss_terms = torch.zeros(7)\n",
    "\t\tfor index, (q, M_q, A_q) in enumerate(train_dataloader):\n",
    "\t\t\tq = q.to(device)\n",
    "\t\t\tM_q = M_q.to(device)\n",
    "\t\t\tA_q = A_q.to(device)\n",
    "\n",
    "\t\t\tbatch_size = q.size(0)\n",
    "\t\t\t\n",
    "\t\t\ttheta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)  \n",
    "\t\t\ttheta_ana = model.theta_ana(q)\n",
    "\t\t\t\t\t\n",
    "\t\t\tloss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\ttrain_loss += loss.item() * batch_size\n",
    "\t\t\ttrain_loss_terms += loss_terms * batch_size\n",
    "\t\ttrain_loss /= len(train_dataloader.dataset) \n",
    "\t\ttrain_loss_terms /= len(train_dataloader.dataset) \n",
    "\t\ttrain_losses.append(train_loss)\n",
    "\n",
    "\t\t# Validation phase\n",
    "\t\tmodel.eval()\n",
    "\t\tval_loss = 0\n",
    "\t\tval_loss_terms = torch.zeros(7)\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor index, (q, M_q, A_q) in enumerate(val_dataloader):\n",
    "\t\t\t\tq = q.to(device)\n",
    "\t\t\t\tM_q = M_q.to(device)\n",
    "\t\t\t\tA_q = A_q.to(device)\n",
    "\n",
    "\t\t\t\tbatch_size = q.size(0)\n",
    "\n",
    "\t\t\t\ttheta, J_h, q_hat, J_h_dec, J_h_ana = model.forward(q)\n",
    "\t\t\t\ttheta_ana = model.theta_ana(q)\n",
    "\n",
    "\t\t\t\tloss, loss_terms = loss_fun(q, theta, q_hat, M_q, A_q, J_h, J_h_dec)\n",
    "\n",
    "\t\t\t\tval_loss += loss.item() * batch_size\n",
    "\t\t\t\tval_loss_terms += loss_terms * batch_size\n",
    "\t\tval_loss /= len(val_dataloader.dataset)\n",
    "\t\tval_loss_terms /= len(val_dataloader.dataset)\n",
    "\t\tval_losses.append(val_loss)\n",
    "\t\tepoch_duration = time.time() - start_time\n",
    "\t\tscheduler.step()\n",
    "\n",
    "\n",
    "\t\t# Print results every epoch\n",
    "\t\ttlt = train_loss_terms\n",
    "\t\tprint(\n",
    "\t\t\tf'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Duration: {epoch_duration:.2f} seconds')\n",
    "\t\tprint(\n",
    "\t\t    f'l_recon: {tlt[0]:.4f}, l_diag: {tlt[1]:.4f}, l_off_diag: {tlt[2]:.4f}, l_input: {tlt[3]:.4f}, l_input_jac: {tlt[4]:.4f}, l_theta: {tlt[5]:.4f}')\n",
    "\t\t#print(\n",
    "\t\t#\tf'l_recon: {tlt[0]:.4f}, l_inertia: {tlt[1]:.4f}, l_input: {tlt[2]:.4f}, l_input_jac: {tlt[3]:.4f}, l_theta: {tlt[4]:.4f}')\n",
    "\t\t\n",
    "\treturn(model, train_losses, val_losses, dir_path, nn_filepath, file_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, file_path):\n",
    "\ttorch.save(model.state_dict(), file_path)\n",
    "\tprint(f\"Model parameters saved to {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c14e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_losses, val_losses, log = False, save = False, file_path = None):\n",
    "\n",
    "\t\"\"\"\n",
    "\tPlots training and validation loss. \n",
    "\tylim\" and \"yscale\" should be enabled depending on the loss function.\n",
    "\t\"\"\"\n",
    "\n",
    "\tplt.figure(figsize=(10, 6))\n",
    "\tplt.plot(train_losses, label=\"Training Loss\")\n",
    "\tplt.plot(val_losses, label=\"Validation Loss\")\n",
    "\tplt.xlabel(\"Epoch\")\n",
    "\tplt.ylabel(\"Loss\")\n",
    "\t#plt.ylim((-1, 40))\n",
    "\tplt.legend()\n",
    "\tplt.title(\"Training and Validation Loss over Epochs\")\n",
    "\tplt.grid(True)\n",
    "\tif log:\n",
    "\t\tplt.yscale(\"log\")\n",
    "\tif save:\n",
    "\t\tplt.savefig(file_path, dpi=800)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d44a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Plotting.plotters_simple as plotters_simple\n",
    "\n",
    "def make_plot_dataloader(dataset, stride = 1):\n",
    "\n",
    "\t\"\"\"\n",
    "\tTakes the training dataset and returns a dataloader of every 10th point\n",
    "\tto reduce visual clutter. \n",
    "\t\"\"\"\n",
    "\n",
    "\tpoints_tensor, mass_matrices_tensor, input_matrices_tensor = dataset.tensors\n",
    "\t\n",
    "\tplot_sampled = points_tensor[::stride]\n",
    "\tmass_sampled = mass_matrices_tensor[::stride]\n",
    "\tinput_sampled = input_matrices_tensor[::stride]\n",
    "\n",
    "\tplot_dataset = TensorDataset(plot_sampled, mass_sampled, input_sampled)\n",
    "\tplot_dataloader = DataLoader(plot_dataset, batch_size=len(plot_dataset), shuffle=False, num_workers=0)\n",
    "\n",
    "\treturn(plot_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c160a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(model, plot_dataloader, save_folder, device):\n",
    "\n",
    "\tos.makedirs(save_folder, exist_ok=True)\n",
    "\n",
    "\tmodel.eval()\n",
    "\twith torch.no_grad():\n",
    "\t\tfor (q, M_q, A_q) in plot_dataloader:\n",
    "\t\t\tq = q.to(device)\n",
    "\t\t\tM_q = M_q.to(device)\n",
    "\t\t\tA_q = A_q.to(device)\n",
    "\n",
    "\t\t\ttheta, J_h, q_hat, J_h_dec, J_h_ana = model(q)\n",
    "\t\t\ttheta_ana = model.theta_ana(q)\n",
    "\t\t\tJ_h_trans = torch.transpose(J_h, 1, 2)\n",
    "\t\t\tJ_h_inv = J_h_dec\n",
    "\t\t\tJ_h_inv_trans = torch.transpose(J_h_inv, 1, 2)\n",
    "\n",
    "\t\t\tJ_h_inv_ana = torch.linalg.inv(J_h_ana)\n",
    "\t\t\tJ_h_inv_trans_ana = torch.transpose(J_h_inv_ana, 1, 2)\n",
    "\n",
    "\t\t\tM_th = J_h_inv_trans @ M_q @ J_h_inv\n",
    "\t\t\tA_th = (J_h_inv_trans @ A_q).squeeze(-1)\n",
    "\n",
    "\t\t\tM_th_ana = J_h_inv_trans_ana @ M_q @ J_h_inv_ana\n",
    "\t\t\tA_th_ana = (J_h_inv_trans_ana @ A_q).squeeze(-1)\n",
    "\n",
    "\t\t\t\n",
    "\t\t\tplotters_simple.plot_3d_quad(q, [theta_ana[:, 0], theta[:, 0], theta_ana[:, 1], theta[:, 1]], \"Analytic vs learned theta\", \n",
    "\t\t\t\t\t\t\t\t\t\t [\"th0_ana\", \"th0_learned\", \"th_1_ana\", \"th_1_learned\"], \"q_0\", \"q_1\", \"th\", folder_path)\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\t\t\tplotters_simple.plot_3d_double(q, [A_th[:, 0], A_th[:, 1]], \"Input decoupling\", [\"A0\", \"A1\"], \"q_0\", \"q_1\", \"A\", folder_path)\n",
    "\t\t\t\n",
    "\t\t\tA_th = A_th.cpu().detach().numpy()\n",
    "\t\t\tprint(\"Percentage of abs(A_0) > 0.6:\", 100 * np.sum(np.abs(A_th[:, 0]) > 0.6)/A_th[:, 0].size, \"%\")\n",
    "\t\t\tprint(\"Percentage of abs(A_1) < 0.3:\", 100 * np.sum(np.abs(A_th[:, 1]) < 0.3)/A_th[:, 1].size, \"%\")\n",
    "\t\t\t\n",
    "\t\t\tplotters_simple.plot_3d_quad(q, [M_th[:, 0, 0], M_th[:, 0, 1], M_th[:, 1, 0], M_th[:, 1, 1]], \"M_th vs q\", \n",
    "\t\t\t\t\t\t\t\t\t\t [\"M_th[0,0]\", \"M_th[0,1]\", \"M_th[1,0]\", \"M_th[1,1]\"], \"q_0\", \"q_1\", \"M_th\", folder_path)\n",
    "\t\t\tM_th_cpu = M_th.cpu().detach().numpy()\n",
    "\t\t\tprint(\"Percentage of abs(M_00) > 1.0:\", 100 * np.sum(np.abs(M_th_cpu[:, 0, 0]) > 1.0)/M_th_cpu[:, 0, 0].size, \"%\")\n",
    "\t\t\tprint(\"Percentage of abs(M_01) < 0.2:\", 100 * np.sum(np.abs(M_th_cpu[:, 0, 1]) < 0.2)/M_th_cpu[:, 0, 1].size, \"%\")\n",
    "\t\t\tprint(\"Percentage of abs(M_11) > 1.0:\", 100 * np.sum(np.abs(M_th_cpu[:, 1, 1]) > 1.0)/M_th_cpu[:, 1, 1].size, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81624d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1_split = (-torch.pi, torch.pi)\n",
    "batch_size = 512\n",
    "train_part = 0.7\n",
    "\n",
    "num_epochs = 1001\n",
    "lr = 1e-3\n",
    "\n",
    "# IMPORTANT:\n",
    "# If you want to train on an existing model\n",
    "existing_model_name = \"NN_202503231956\"\n",
    "existing_model_name = None\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
    "\n",
    "dataset = make_dataset(points)\n",
    "(train_dataloader, val_dataloader) = make_dataloaders(dataset=dataset, batch_size=batch_size, train_part=train_part)\n",
    "outputs = []\n",
    "model, train_losses, val_losses, dir_path, nn_filepath, file_counter = train_AE_model(rp, device, lr, num_epochs, q1_split, train_dataloader, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\tval_dataloader, current_time, existing_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec749cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_path = os.path.join(dir_path, \"training_points_\" + str(file_counter) + \".png\")\n",
    "points_plotter(points, save = True, file_path = points_path)\n",
    "scatter_fig_path = os.path.join(dir_path, \"loss_\" + str(file_counter) + \".png\")\n",
    "plot_loss(train_losses, val_losses, log = False, save = True, file_path = scatter_fig_path)\n",
    "save_model(model, nn_filepath)\n",
    "print(nn_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def theta_1_single(model, q):\n",
    "\tprint(\"q:\", q.size())\n",
    "\ttheta = model.encoder(q)\n",
    "\tprint(\"theta:\", theta.size())\n",
    "\treturn theta[:, 0].detach()\n",
    "\t\n",
    "def theta_2_single(model, q):\n",
    "\ttheta = model.encoder(q)\n",
    "\treturn theta[:, 1].detach()\n",
    "\t\n",
    "def q_hat_1_single(model, theta):\n",
    "\tq_hat = model.decoder(theta)\n",
    "\treturn q_hat[:, 0].detach()\n",
    "\t\n",
    "def q_hat_2_single(model, theta):\n",
    "\tq_hat = model.decoder(theta)\n",
    "\treturn q_hat[:, 1].detach()\n",
    "\n",
    "mapping_functions = (partial(theta_1_single,model), \n",
    "\t\t\t\t\t\tpartial(theta_2_single,model), \n",
    "\t\t\t\t\t\tpartial(q_hat_1_single,model), \n",
    "\t\t\t\t\t\tpartial(q_hat_2_single,model))\n",
    "th_plotter = theta_visualizer.theta_plotter(rp=rp, n_lines=50, device=device, \n",
    "\t\t\t\t\t\t\t\t\t\t\tmapping_functions=mapping_functions, mask_split=q1_split)\n",
    "theta_figure_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".png\")\n",
    "th_plotter.make_figure(theta_figure_path)\n",
    "theta_anim_path = os.path.join(dir_path, \"theta_mapping_\" + str(file_counter) + \".mp4\")\n",
    "#th_plotter.make_animation(theta_anim_path, duration = 4, fps = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9b45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "folder_path = os.path.join(dir_path, \"performance_plots_\" + str(file_counter))\n",
    "\n",
    "q1_split = (-torch.pi, torch.pi)\n",
    "plt.ion()\n",
    "#model = autoencoders.Autoencoder_double(rp).to(device)\n",
    "#model_location = 'Models/Split_AEs/Lumped_Mass_202503051257.pth'\n",
    "#model.load_state_dict(torch.load(model_location, weights_only=True))\n",
    "plot_points = points\n",
    "plot_dataset = make_dataset(plot_points)\n",
    "plot_dataloader = make_plot_dataloader(plot_dataset)\n",
    "plot_model_performance(model, plot_dataloader, folder_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
